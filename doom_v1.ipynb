{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMnU6W5xAuUH+DWgsdBZ5CC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccasanoval/RLtests/blob/master/doom_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning : Doom : v1"
      ],
      "metadata": {
        "id": "iHhZXGGQiMBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vizdoom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spuXbhqJihu-",
        "outputId": "8d0eab88-ffba-40bb-fd1e-c0b6844ff4d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vizdoom in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vizdoom) (1.25.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (0.29.1)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from vizdoom import *\n",
        "import timeit\n",
        "import math\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "GxgzgMnKiHr2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imput shape"
      ],
      "metadata": {
        "id": "0TBBOtDsjHxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_shape(Image,Filter,Stride):\n",
        "    layer1 = math.ceil(((Image - Filter + 1) / Stride))\n",
        "\n",
        "    o1 = math.ceil((layer1 / Stride))\n",
        "\n",
        "    layer2 = math.ceil(((o1 - Filter + 1) / Stride))\n",
        "\n",
        "    o2 = math.ceil((layer2 / Stride))\n",
        "\n",
        "    layer3 = math.ceil(((o2 - Filter + 1) / Stride))\n",
        "\n",
        "    o3 = math.ceil((layer3  / Stride))\n",
        "\n",
        "    return int(o3)"
      ],
      "metadata": {
        "id": "lE4ADOpiiwNo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DRQN: Deep Recurrent Q Network"
      ],
      "metadata": {
        "id": "zmvVj7xti5xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DRQN():\n",
        "    def __init__(self, input_shape, num_actions, inital_learning_rate):\n",
        "\n",
        "        # first, we initialize all the hyperparameters\n",
        "        self.tfcast_type = tf.float32\n",
        "\n",
        "        # shape of our input which would be (length, width, channels)\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        # number of actions in the environment\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # learning rate for the neural network\n",
        "        self.learning_rate = inital_learning_rate\n",
        "\n",
        "        # now we will define the hyperparameters of the convolutional neural network\n",
        "\n",
        "        # filter size\n",
        "        self.filter_size = 5\n",
        "\n",
        "        # number of filters\n",
        "        self.num_filters = [16, 32, 64]\n",
        "\n",
        "        # stride size\n",
        "        self.stride = 2\n",
        "\n",
        "        # pool size\n",
        "        self.poolsize = 2\n",
        "\n",
        "        # shape of our convolutional layer\n",
        "        self.convolution_shape = get_input_shape(input_shape[0], self.filter_size, self.stride) * get_input_shape(input_shape[1], self.filter_size, self.stride) * self.num_filters[2]\n",
        "\n",
        "        # now we define the hyperparameters of our recurrent neural network and the final feed forward layer\n",
        "\n",
        "        # number of neurons\n",
        "        self.cell_size = 100\n",
        "\n",
        "        # number of hidden layers\n",
        "        self.hidden_layer = 50\n",
        "\n",
        "        # drop out probability\n",
        "        self.dropout_probability = [0.3, 0.2]\n",
        "\n",
        "        # hyperparameters for optimization\n",
        "        self.loss_decay_rate = 0.96\n",
        "        self.loss_decay_steps = 180\n",
        "\n",
        "\n",
        "        # initialize all the variables for the CNN\n",
        "\n",
        "        # we initialize the placeholder for input whose shape would be (length, width, channel)\n",
        "        self.input = tf.placeholder(shape = (self.input_shape[0], self.input_shape[1], self.input_shape[2]), dtype = self.tfcast_type)\n",
        "\n",
        "        # we will also initialize the shape of the target vector whose shape is equal to the number of actions\n",
        "        self.target_vector = tf.placeholder(shape = (self.num_actions, 1), dtype = self.tfcast_type)\n",
        "\n",
        "        # initialize feature maps for our corresponding 3 filters\n",
        "        self.features1 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, input_shape[2], self.num_filters[0]),\n",
        "                                     dtype = self.tfcast_type)\n",
        "\n",
        "        self.features2 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, self.num_filters[0], self.num_filters[1]),\n",
        "                                     dtype = self.tfcast_type)\n",
        "\n",
        "\n",
        "        self.features3 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, self.num_filters[1], self.num_filters[2]),\n",
        "                                     dtype = self.tfcast_type)\n",
        "\n",
        "        # initialize variables for RNN\n",
        "        # recall how RNN works from chapter 7\n",
        "\n",
        "        self.h = tf.Variable(initial_value = np.zeros((1, self.cell_size)), dtype = self.tfcast_type)\n",
        "\n",
        "        # hidden to hidden weight matrix\n",
        "        self.rW = tf.Variable(initial_value = np.random.uniform(\n",
        "                                            low = -np.sqrt(6. / (self.convolution_shape + self.cell_size)),\n",
        "                                            high = np.sqrt(6. / (self.convolution_shape + self.cell_size)),\n",
        "                                            size = (self.convolution_shape, self.cell_size)),\n",
        "                              dtype = self.tfcast_type)\n",
        "\n",
        "        # input to hidden weight matrix\n",
        "        self.rU = tf.Variable(initial_value = np.random.uniform(\n",
        "                                            low = -np.sqrt(6. / (2 * self.cell_size)),\n",
        "                                            high = np.sqrt(6. / (2 * self.cell_size)),\n",
        "                                            size = (self.cell_size, self.cell_size)),\n",
        "                              dtype = self.tfcast_type)\n",
        "\n",
        "        # hiddent to output weight matrix\n",
        "\n",
        "        self.rV = tf.Variable(initial_value = np.random.uniform(\n",
        "                                            low = -np.sqrt(6. / (2 * self.cell_size)),\n",
        "                                            high = np.sqrt(6. / (2 * self.cell_size)),\n",
        "                                            size = (self.cell_size, self.cell_size)),\n",
        "                              dtype = self.tfcast_type)\n",
        "        # bias\n",
        "        self.rb = tf.Variable(initial_value = np.zeros(self.cell_size), dtype = self.tfcast_type)\n",
        "        self.rc = tf.Variable(initial_value = np.zeros(self.cell_size), dtype = self.tfcast_type)\n",
        "\n",
        "\n",
        "        # initialize weights and bias of feed forward network\n",
        "\n",
        "        # weights\n",
        "        self.fW = tf.Variable(initial_value = np.random.uniform(\n",
        "                                            low = -np.sqrt(6. / (self.cell_size + self.num_actions)),\n",
        "                                            high = np.sqrt(6. / (self.cell_size + self.num_actions)),\n",
        "                                            size = (self.cell_size, self.num_actions)),\n",
        "                              dtype = self.tfcast_type)\n",
        "\n",
        "        # bias\n",
        "        self.fb = tf.Variable(initial_value = np.zeros(self.num_actions), dtype = self.tfcast_type)\n",
        "\n",
        "        # learning rate\n",
        "        self.step_count = tf.Variable(initial_value = 0, dtype = self.tfcast_type)\n",
        "        self.learning_rate = tf.train.exponential_decay(self.learning_rate,\n",
        "                                                   self.step_count,\n",
        "                                                   self.loss_decay_steps,\n",
        "                                                   self.loss_decay_steps,\n",
        "                                                   staircase = False)\n",
        "\n",
        "\n",
        "        # now let us build the network\n",
        "\n",
        "        # first convolutional layer\n",
        "        self.conv1 = tf.nn.conv2d(input = tf.reshape(self.input, shape = (1, self.input_shape[0], self.input_shape[1], self.input_shape[2])), filter = self.features1, strides = [1, self.stride, self.stride, 1], padding = \"VALID\")\n",
        "        self.relu1 = tf.nn.relu(self.conv1)\n",
        "        self.pool1 = tf.nn.max_pool(self.relu1, ksize = [1, self.poolsize, self.poolsize, 1], strides = [1, self.stride, self.stride, 1], padding = \"SAME\")\n",
        "\n",
        "        # second convolutional layer\n",
        "        self.conv2 = tf.nn.conv2d(input = self.pool1, filter = self.features2, strides = [1, self.stride, self.stride, 1], padding = \"VALID\")\n",
        "        self.relu2 = tf.nn.relu(self.conv2)\n",
        "        self.pool2 = tf.nn.max_pool(self.relu2, ksize = [1, self.poolsize, self.poolsize, 1], strides = [1, self.stride, self.stride, 1], padding = \"SAME\")\n",
        "\n",
        "        # third convolutional layer\n",
        "        self.conv3 = tf.nn.conv2d(input = self.pool2, filter = self.features3, strides = [1, self.stride, self.stride, 1], padding = \"VALID\")\n",
        "        self.relu3 = tf.nn.relu(self.conv3)\n",
        "        self.pool3 = tf.nn.max_pool(self.relu3, ksize = [1, self.poolsize, self.poolsize, 1], strides = [1, self.stride, self.stride, 1], padding = \"SAME\")\n",
        "\n",
        "        # add dropout and reshape the input\n",
        "        self.drop1 = tf.nn.dropout(self.pool3, self.dropout_probability[0])\n",
        "        self.reshaped_input = tf.reshape(self.drop1, shape = [1, -1])\n",
        "\n",
        "\n",
        "        # now we build recurrent neural network which takes the input from the last layer of convolutional network\n",
        "        self.h = tf.tanh(tf.matmul(self.reshaped_input, self.rW) + tf.matmul(self.h, self.rU) + self.rb)\n",
        "        self.o = tf.nn.softmax(tf.matmul(self.h, self.rV) + self.rc)\n",
        "\n",
        "        # add drop out to RNN\n",
        "        self.drop2 = tf.nn.dropout(self.o, self.dropout_probability[1])\n",
        "\n",
        "        # we feed the result of RNN to the feed forward layer\n",
        "        self.output = tf.reshape(tf.matmul(self.drop2, self.fW) + self.fb, shape = [-1, 1])\n",
        "        self.prediction = tf.argmax(self.output)\n",
        "\n",
        "        # compute loss\n",
        "        self.loss = tf.reduce_mean(tf.square(self.target_vector - self.output))\n",
        "\n",
        "        # we use Adam optimizer for minimizing the error\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "\n",
        "        # compute gradients of the loss and update the gradients\n",
        "        self.gradients = self.optimizer.compute_gradients(self.loss)\n",
        "        self.update = self.optimizer.apply_gradients(self.gradients)\n",
        "\n",
        "        self.parameters = (self.features1, self.features2, self.features3,\n",
        "                           self.rW, self.rU, self.rV, self.rb, self.rc,\n",
        "                           self.fW, self.fb)\n",
        ""
      ],
      "metadata": {
        "id": "N0PGC2MRi3fB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experience replay"
      ],
      "metadata": {
        "id": "58c3e5qwjPEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplay():\n",
        "    def __init__(self, buffer_size):\n",
        "\n",
        "        # buffer for holding the transistion\n",
        "        self.buffer = []\n",
        "\n",
        "        # size of the buffer\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "    # we remove the old transistion if buffer size has reached it's limit. Think off the buffer as a queue when new\n",
        "    # one comes, old one goes off\n",
        "\n",
        "    def appendToBuffer(self, memory_tuplet):\n",
        "        if len(self.buffer) > self.buffer_size:\n",
        "            for i in range(len(self.buffer) - self.buffer_size):\n",
        "                self.buffer.remove(self.buffer[0])\n",
        "        self.buffer.append(memory_tuplet)\n",
        "\n",
        "\n",
        "    # define a function called sample for sampling some random n number of transistions\n",
        "\n",
        "    def sample(self, n):\n",
        "        memories = []\n",
        "\n",
        "        for i in range(n):\n",
        "            memory_index = np.random.randint(0, len(self.buffer))\n",
        "            memories.append(self.buffer[memory_index])\n",
        "        return memories"
      ],
      "metadata": {
        "id": "m4jROIU_jQyH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training function : we declare some variables outside for testing, clean it later"
      ],
      "metadata": {
        "id": "B1xo_-OTjZvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want this to test later, please clean the test...\n",
        "actionDRQN = DRQN((160, 256, 3), 4, 0.1)\n",
        "targetDRQN = DRQN((160, 256, 3), 4, 0.1)\n",
        "actions = np.zeros((4,4))\n",
        "\n",
        "# How can we import a file from our github?\n",
        "import requests\n",
        "req = requests.get(\"https://raw.githubusercontent.com/ccasanoval/RLtests/master/vizdoom/deathmatch.cfg\")\n",
        "filename = \"deathmatch.cfg\"\n",
        "f = open(filename,'w')\n",
        "f.write(req.text)\n",
        "\n",
        "# Train the DRQN agent ---------------------------------------------------------\n",
        "def train(\n",
        "    num_episodes,\n",
        "    episode_length,\n",
        "    learning_rate,\n",
        "    scenario = \"deathmatch.cfg\",\n",
        "    map_path = 'map02',\n",
        "    render = False\n",
        "    ):\n",
        "\n",
        "    # discount parameter for Q-value computation\n",
        "    discount_factor = .99\n",
        "\n",
        "    # frequency for updating the experience in the buffer\n",
        "    update_frequency = 5\n",
        "    store_frequency = 50\n",
        "\n",
        "    # for printing the output\n",
        "    print_frequency = 1000\n",
        "\n",
        "    # initialize variables for storing total rewards and total loss\n",
        "    total_reward = 0\n",
        "    total_loss = 0\n",
        "    old_q_value = 0\n",
        "\n",
        "    # initialize lists for storing the episodic rewards and losses\n",
        "    rewards = []\n",
        "    losses = []\n",
        "\n",
        "    # okay, now let us get to the action!\n",
        "\n",
        "    # first, we initialize our doomgame environment\n",
        "    game = DoomGame()\n",
        "\n",
        "    # specify the path where our scenario file is located\n",
        "    game.set_doom_scenario_path(scenario)\n",
        "\n",
        "    # specify the path of map file\n",
        "    game.set_doom_map(map_path)\n",
        "\n",
        "    # then we set screen resolution and screen format\n",
        "    game.set_screen_resolution(ScreenResolution.RES_256X160)\n",
        "    game.set_screen_format(ScreenFormat.RGB24)\n",
        "\n",
        "    # we can add particles and effetcs we needed by simply setting them to true or false\n",
        "    game.set_render_hud(False)\n",
        "    game.set_render_minimal_hud(False)\n",
        "    game.set_render_crosshair(False)\n",
        "    game.set_render_weapon(True)\n",
        "    game.set_render_decals(False)\n",
        "    game.set_render_particles(False)\n",
        "    game.set_render_effects_sprites(False)\n",
        "    game.set_render_messages(False)\n",
        "    game.set_render_corpses(False)\n",
        "    game.set_render_screen_flashes(True)\n",
        "\n",
        "    # now we will specify buttons that should be available to the agent\n",
        "    game.add_available_button(Button.MOVE_LEFT)\n",
        "    game.add_available_button(Button.MOVE_RIGHT)\n",
        "    game.add_available_button(Button.TURN_LEFT)\n",
        "    game.add_available_button(Button.TURN_RIGHT)\n",
        "    game.add_available_button(Button.MOVE_FORWARD)\n",
        "    game.add_available_button(Button.MOVE_BACKWARD)\n",
        "    game.add_available_button(Button.ATTACK)\n",
        "\n",
        "\n",
        "    # okay,now we will add one more button called delta. The above button will only work\n",
        "    # like a keyboard keys and will have only boolean values.\n",
        "\n",
        "    # so we use delta button which emulates a mouse device which will have positive and negative values\n",
        "    # and it will be useful in environment for exploring\n",
        "\n",
        "    game.add_available_button(Button.TURN_LEFT_RIGHT_DELTA, 90)\n",
        "    game.add_available_button(Button.LOOK_UP_DOWN_DELTA, 90)\n",
        "\n",
        "    # initialize an array for actions\n",
        "    actions = np.zeros((game.get_available_buttons_size(), game.get_available_buttons_size()))\n",
        "    count = 0\n",
        "    for i in actions:\n",
        "        i[count] = 1\n",
        "        count += 1\n",
        "    actions = actions.astype(int).tolist()\n",
        "\n",
        "\n",
        "    # then we add the game variables, ammo, health, and killcount\n",
        "    game.add_available_game_variable(GameVariable.AMMO0)\n",
        "    game.add_available_game_variable(GameVariable.HEALTH)\n",
        "    game.add_available_game_variable(GameVariable.KILLCOUNT)\n",
        "\n",
        "    # we set episode_timeout to terminate the episode after some time step\n",
        "    # we also set episode_start_time which is useful for skipping intial events\n",
        "\n",
        "    game.set_episode_timeout(6 * episode_length)\n",
        "    game.set_episode_start_time(10)\n",
        "    game.set_window_visible(render)\n",
        "\n",
        "    # we can also enable sound by setting set_sound_enable to true\n",
        "    game.set_sound_enabled(False)\n",
        "\n",
        "    # we set living reward to 0 which the agent for each move it does even though the move is not useful\n",
        "    game.set_living_reward(0)\n",
        "\n",
        "    # doom has different modes such as player, spectator, asynchronous player and asynchronous spectator\n",
        "\n",
        "    # in spectator mode humans will play and agent will learn from it.\n",
        "    # in player mode, agent actually plays the game, so we use player mode.\n",
        "\n",
        "    game.set_mode(Mode.PLAYER)\n",
        "\n",
        "    # okay, So now we, initialize the game environment\n",
        "    game.init()\n",
        "\n",
        "    # now, let us create instance to our DRQN class and create our both actor and target DRQN networks\n",
        "    actionDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)\n",
        "    targetDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)\n",
        "\n",
        "    # we will also create instance to the ExperienceReplay class with the buffer size of 1000\n",
        "    experiences = ExperienceReplay(1000)\n",
        "\n",
        "    # for storing the models\n",
        "    saver = tf.train.Saver({v.name: v for v in actionDRQN.parameters}, max_to_keep = 1)\n",
        "\n",
        "\n",
        "    # now let us start the training process\n",
        "    # we initialize variables for sampling and storing transistions from the experience buffer\n",
        "    sample = 5\n",
        "    store = 50\n",
        "\n",
        "    # start the tensorflow session\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        # initialize all tensorflow variables\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "\n",
        "            # start the new episode\n",
        "            game.new_episode()\n",
        "\n",
        "            # play the episode till it reaches the episode length\n",
        "            for frame in range(episode_length):\n",
        "\n",
        "                # get the game state\n",
        "                state = game.get_state()\n",
        "                s = state.screen_buffer\n",
        "\n",
        "                # select the action\n",
        "                a = actionDRQN.prediction.eval(feed_dict = {actionDRQN.input: s})[0]\n",
        "                action = actions[a]\n",
        "\n",
        "                # perform the action and store the reward\n",
        "                reward = game.make_action(action)\n",
        "\n",
        "                # update total rewad\n",
        "                total_reward += reward\n",
        "\n",
        "\n",
        "                # if the episode is over then break\n",
        "                if game.is_episode_finished():\n",
        "                    break\n",
        "\n",
        "                # store transistion to our experience buffer\n",
        "                if (frame % store) == 0:\n",
        "                    experiences.appendToBuffer((s, action, reward))\n",
        "\n",
        "                # sample experience form the experience buffer\n",
        "                if (frame % sample) == 0:\n",
        "                    memory = experiences.sample(1)\n",
        "                    mem_frame = memory[0][0]\n",
        "                    mem_reward = memory[0][2]\n",
        "\n",
        "\n",
        "                    # now, train the network\n",
        "                    Q1 = actionDRQN.output.eval(feed_dict = {actionDRQN.input: mem_frame})\n",
        "                    Q2 = targetDRQN.output.eval(feed_dict = {targetDRQN.input: mem_frame})\n",
        "\n",
        "                    # set learning rate\n",
        "                    learning_rate = actionDRQN.learning_rate.eval()\n",
        "\n",
        "                    # calculate Q value\n",
        "                    Qtarget = old_q_value + learning_rate * (mem_reward + discount_factor * Q2 - old_q_value)\n",
        "\n",
        "                    # update old Q value\n",
        "                    old_q_value = Qtarget\n",
        "\n",
        "                    # compute Loss\n",
        "                    loss = actionDRQN.loss.eval(feed_dict = {actionDRQN.target_vector: Qtarget, actionDRQN.input: mem_frame})\n",
        "\n",
        "                    # update total loss\n",
        "                    total_loss += loss\n",
        "\n",
        "                    # update both networks\n",
        "                    actionDRQN.update.run(feed_dict = {actionDRQN.target_vector: Qtarget, actionDRQN.input: mem_frame})\n",
        "                    targetDRQN.update.run(feed_dict = {targetDRQN.target_vector: Qtarget, targetDRQN.input: mem_frame})\n",
        "\n",
        "            rewards.append((episode, total_reward))\n",
        "            losses.append((episode, total_loss))\n",
        "\n",
        "\n",
        "            print(\"Episode %d - Reward = %.3f, Loss = %.3f.\" % (episode, total_reward, total_loss))\n",
        "\n",
        "\n",
        "            total_reward = 0\n",
        "            total_loss = 0"
      ],
      "metadata": {
        "id": "5AgbYNYVjmVY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "I1bV3xQmjnt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(\"Start Time: \", datetime.now().strftime(\"%H:%M:%S\"))\n",
        "import time\n",
        "start = time.process_time()\n",
        "\n",
        "train(num_episodes = 10000, episode_length = 300, learning_rate = 0.01, render = False)\n",
        "\n",
        "print(\"Stop Time: \", datetime.now().strftime(\"%H:%M:%S\"))\n",
        "print(\"Total training time: \", (time.process_time() - start)/60, \" min\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtHZWJVvjphL",
        "outputId": "c37d969f-453e-4011-b0a1-71fd0496cd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Time:  13:32:12\n",
            "Episode 0 - Reward = 0.000, Loss = 0.346.\n",
            "Episode 1 - Reward = 0.000, Loss = 0.404.\n",
            "Episode 2 - Reward = 0.000, Loss = 0.196.\n",
            "Episode 3 - Reward = 0.000, Loss = 0.258.\n",
            "Episode 4 - Reward = 0.000, Loss = 0.286.\n",
            "Episode 5 - Reward = 0.000, Loss = 0.069.\n",
            "Episode 6 - Reward = 0.000, Loss = 0.031.\n",
            "Episode 7 - Reward = 0.000, Loss = 0.041.\n",
            "Episode 8 - Reward = 0.000, Loss = 0.382.\n",
            "Episode 9 - Reward = 0.000, Loss = 0.028.\n",
            "Episode 10 - Reward = 0.000, Loss = 0.035.\n",
            "Episode 11 - Reward = 0.000, Loss = 0.057.\n",
            "Episode 12 - Reward = 0.000, Loss = 0.013.\n",
            "Episode 13 - Reward = 0.000, Loss = 0.057.\n",
            "Episode 14 - Reward = 0.000, Loss = 0.023.\n",
            "Episode 15 - Reward = 0.000, Loss = 0.141.\n",
            "Episode 16 - Reward = 0.000, Loss = 0.074.\n",
            "Episode 17 - Reward = 0.000, Loss = 0.098.\n",
            "Episode 18 - Reward = 0.000, Loss = 0.013.\n",
            "Episode 19 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 20 - Reward = 0.000, Loss = 0.025.\n",
            "Episode 21 - Reward = 0.000, Loss = 0.051.\n",
            "Episode 22 - Reward = 0.000, Loss = 0.031.\n",
            "Episode 23 - Reward = 0.000, Loss = 0.031.\n",
            "Episode 24 - Reward = 0.000, Loss = 0.060.\n",
            "Episode 25 - Reward = 0.000, Loss = 0.013.\n",
            "Episode 26 - Reward = 0.000, Loss = 0.007.\n",
            "Episode 27 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 28 - Reward = 0.000, Loss = 0.040.\n",
            "Episode 29 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 30 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 31 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 32 - Reward = 0.000, Loss = 0.007.\n",
            "Episode 33 - Reward = 0.000, Loss = 0.031.\n",
            "Episode 34 - Reward = 0.000, Loss = 0.143.\n",
            "Episode 35 - Reward = 0.000, Loss = 0.077.\n",
            "Episode 36 - Reward = 0.000, Loss = 0.243.\n",
            "Episode 37 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 38 - Reward = 0.000, Loss = 0.009.\n",
            "Episode 39 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 40 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 41 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 42 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 43 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 44 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 45 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 46 - Reward = 0.000, Loss = 0.011.\n",
            "Episode 47 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 48 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 49 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 50 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 51 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 52 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 53 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 54 - Reward = 0.000, Loss = 0.015.\n",
            "Episode 55 - Reward = 0.000, Loss = 0.117.\n",
            "Episode 56 - Reward = 0.000, Loss = 0.033.\n",
            "Episode 57 - Reward = 0.000, Loss = 0.062.\n",
            "Episode 58 - Reward = 0.000, Loss = 0.018.\n",
            "Episode 59 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 60 - Reward = 0.000, Loss = 0.011.\n",
            "Episode 61 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 62 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 63 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 64 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 65 - Reward = 0.000, Loss = 0.016.\n",
            "Episode 66 - Reward = 0.000, Loss = 0.012.\n",
            "Episode 67 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 68 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 69 - Reward = 0.000, Loss = 0.069.\n",
            "Episode 70 - Reward = 0.000, Loss = 0.530.\n",
            "Episode 71 - Reward = 0.000, Loss = 0.040.\n",
            "Episode 72 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 73 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 74 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 75 - Reward = 0.000, Loss = 0.012.\n",
            "Episode 76 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 77 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 78 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 79 - Reward = 0.000, Loss = 0.051.\n",
            "Episode 80 - Reward = 0.000, Loss = 0.046.\n",
            "Episode 81 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 82 - Reward = 0.000, Loss = 0.464.\n",
            "Episode 83 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 84 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 85 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 86 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 87 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 88 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 89 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 90 - Reward = 0.000, Loss = 0.014.\n",
            "Episode 91 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 92 - Reward = 0.000, Loss = 0.060.\n",
            "Episode 93 - Reward = 0.000, Loss = 0.044.\n",
            "Episode 94 - Reward = 0.000, Loss = 0.074.\n",
            "Episode 95 - Reward = 0.000, Loss = 0.007.\n",
            "Episode 96 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 97 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 98 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 99 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 100 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 101 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 102 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 103 - Reward = 0.000, Loss = 0.022.\n",
            "Episode 104 - Reward = 0.000, Loss = 0.015.\n",
            "Episode 105 - Reward = 0.000, Loss = 0.012.\n",
            "Episode 106 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 107 - Reward = 0.000, Loss = 0.006.\n",
            "Episode 108 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 109 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 110 - Reward = 0.000, Loss = 0.014.\n",
            "Episode 111 - Reward = 0.000, Loss = 0.173.\n",
            "Episode 112 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 113 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 114 - Reward = 0.000, Loss = 0.019.\n",
            "Episode 115 - Reward = 0.000, Loss = 0.089.\n",
            "Episode 116 - Reward = 0.000, Loss = 0.124.\n",
            "Episode 117 - Reward = 0.000, Loss = 0.162.\n",
            "Episode 118 - Reward = 0.000, Loss = 0.302.\n",
            "Episode 119 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 120 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 121 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 122 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 123 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 124 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 125 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 126 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 127 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 128 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 129 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 130 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 131 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 132 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 133 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 134 - Reward = 0.000, Loss = 0.006.\n",
            "Episode 135 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 136 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 137 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 138 - Reward = 0.000, Loss = 0.027.\n",
            "Episode 139 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 140 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 141 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 142 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 143 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 144 - Reward = 0.000, Loss = 0.014.\n",
            "Episode 145 - Reward = 0.000, Loss = 0.027.\n",
            "Episode 146 - Reward = 0.000, Loss = 0.038.\n",
            "Episode 147 - Reward = 0.000, Loss = 0.011.\n",
            "Episode 148 - Reward = 0.000, Loss = 0.024.\n",
            "Episode 149 - Reward = 0.000, Loss = 0.214.\n",
            "Episode 150 - Reward = 0.000, Loss = 0.386.\n",
            "Episode 151 - Reward = 0.000, Loss = 0.060.\n",
            "Episode 152 - Reward = 0.000, Loss = 0.027.\n",
            "Episode 153 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 154 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 155 - Reward = 0.000, Loss = 0.064.\n",
            "Episode 156 - Reward = 0.000, Loss = 0.029.\n",
            "Episode 157 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 158 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 159 - Reward = 0.000, Loss = 0.007.\n",
            "Episode 160 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 161 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 162 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 163 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 164 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 165 - Reward = 0.000, Loss = 0.009.\n",
            "Episode 166 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 167 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 168 - Reward = 0.000, Loss = 0.008.\n",
            "Episode 169 - Reward = 0.000, Loss = 0.009.\n",
            "Episode 170 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 171 - Reward = 0.000, Loss = 0.009.\n",
            "Episode 172 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 173 - Reward = 0.000, Loss = 0.051.\n",
            "Episode 174 - Reward = 0.000, Loss = 0.065.\n",
            "Episode 175 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 176 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 177 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 178 - Reward = 0.000, Loss = 0.842.\n",
            "Episode 179 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 180 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 181 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 182 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 183 - Reward = 0.000, Loss = 0.034.\n",
            "Episode 184 - Reward = 0.000, Loss = 0.058.\n",
            "Episode 185 - Reward = 0.000, Loss = 0.083.\n",
            "Episode 186 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 187 - Reward = 0.000, Loss = 0.022.\n",
            "Episode 188 - Reward = 0.000, Loss = 0.409.\n",
            "Episode 189 - Reward = 0.000, Loss = 0.044.\n",
            "Episode 190 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 191 - Reward = 0.000, Loss = 0.011.\n",
            "Episode 192 - Reward = 0.000, Loss = 0.009.\n",
            "Episode 193 - Reward = 0.000, Loss = 0.078.\n",
            "Episode 194 - Reward = 0.000, Loss = 0.010.\n",
            "Episode 195 - Reward = 0.000, Loss = 0.009.\n",
            "Episode 196 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 197 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 198 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 199 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 200 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 201 - Reward = 0.000, Loss = 0.147.\n",
            "Episode 202 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 203 - Reward = 0.000, Loss = 0.019.\n",
            "Episode 204 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 205 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 206 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 207 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 208 - Reward = 0.000, Loss = 0.010.\n",
            "Episode 209 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 210 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 211 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 212 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 213 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 214 - Reward = 0.000, Loss = 0.006.\n",
            "Episode 215 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 216 - Reward = 0.000, Loss = 0.038.\n",
            "Episode 217 - Reward = 0.000, Loss = 0.016.\n",
            "Episode 218 - Reward = 0.000, Loss = 0.407.\n",
            "Episode 219 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 220 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 221 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 222 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 223 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 224 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 225 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 226 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 227 - Reward = 0.000, Loss = 0.005.\n",
            "Episode 228 - Reward = 0.000, Loss = 0.007.\n",
            "Episode 229 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 230 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 231 - Reward = 0.000, Loss = 0.017.\n",
            "Episode 232 - Reward = 0.000, Loss = 0.013.\n",
            "Episode 233 - Reward = 0.000, Loss = 0.323.\n",
            "Episode 234 - Reward = 0.000, Loss = 0.502.\n",
            "Episode 235 - Reward = 0.000, Loss = 0.134.\n",
            "Episode 236 - Reward = 0.000, Loss = 0.008.\n",
            "Episode 237 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 238 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 239 - Reward = 0.000, Loss = 0.163.\n",
            "Episode 240 - Reward = 0.000, Loss = 0.015.\n",
            "Episode 241 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 242 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 243 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 244 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 245 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 246 - Reward = 0.000, Loss = 0.046.\n",
            "Episode 247 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 248 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 249 - Reward = 0.000, Loss = 0.089.\n",
            "Episode 250 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 251 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 252 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 253 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 254 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 255 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 256 - Reward = 0.000, Loss = 0.017.\n",
            "Episode 257 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 258 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 259 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 260 - Reward = 0.000, Loss = 0.017.\n",
            "Episode 261 - Reward = 0.000, Loss = 0.014.\n",
            "Episode 262 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 263 - Reward = 0.000, Loss = 0.018.\n",
            "Episode 264 - Reward = 0.000, Loss = 0.008.\n",
            "Episode 265 - Reward = 0.000, Loss = 0.298.\n",
            "Episode 266 - Reward = 0.000, Loss = 0.096.\n",
            "Episode 267 - Reward = 0.000, Loss = 0.033.\n",
            "Episode 268 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 269 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 270 - Reward = 0.000, Loss = 0.058.\n",
            "Episode 271 - Reward = 0.000, Loss = 0.010.\n",
            "Episode 272 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 273 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 274 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 275 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 276 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 277 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 278 - Reward = 0.000, Loss = 0.021.\n",
            "Episode 279 - Reward = 0.000, Loss = 0.015.\n",
            "Episode 280 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 281 - Reward = 0.000, Loss = 0.001.\n",
            "Episode 282 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 283 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 284 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 285 - Reward = 0.000, Loss = 0.062.\n",
            "Episode 286 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 287 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 288 - Reward = 0.000, Loss = 0.197.\n",
            "Episode 289 - Reward = 0.000, Loss = 0.034.\n",
            "Episode 290 - Reward = 0.000, Loss = 0.144.\n",
            "Episode 291 - Reward = 0.000, Loss = 0.003.\n",
            "Episode 292 - Reward = 0.000, Loss = 0.009.\n",
            "Episode 293 - Reward = 0.000, Loss = 0.002.\n",
            "Episode 294 - Reward = 0.000, Loss = 0.012.\n",
            "Episode 295 - Reward = 0.000, Loss = 0.016.\n",
            "Episode 296 - Reward = 0.000, Loss = 0.012.\n",
            "Episode 297 - Reward = 0.000, Loss = 0.049.\n",
            "Episode 298 - Reward = 0.000, Loss = 0.004.\n",
            "Episode 299 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 300 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 301 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 302 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 303 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 304 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 305 - Reward = 0.000, Loss = 0.000.\n",
            "Episode 306 - Reward = 0.000, Loss = 0.000.\n"
          ]
        }
      ]
    }
  ]
}