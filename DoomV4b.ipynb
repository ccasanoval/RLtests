{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccasanoval/RLtests/blob/master/DoomV4b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1PChgxjcgnW"
      },
      "source": [
        "#DoomV4b\n",
        "\n",
        "RL  = Stable Baseline 3 : PPO\n",
        "\n",
        "ENV = Gymnasium + VizDoom\n",
        "\n",
        "URL = https://github.com/AKapich/Reinforcement_Learning_Doom\n",
        "\n",
        "URL = https://github.com/AKapich/Reinforcement_Learning_Doom/blob/main/MyWayHome/my_way_home_env.py\n",
        "\n",
        "URL = https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "\n",
        "###.\n",
        "### ALSO TEST : RLlib\n",
        "\n",
        "###.\n",
        "###Â ALSTO TRY: callbacks\n",
        "\n",
        "https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/4_callbacks_hyperparameter_tuning.ipynb#scrollTo=adsKMvDkRUn0\n",
        "\n",
        "###.\n",
        "### ALSO READ: Algorithm Distilation + In Context RL\n",
        "\n",
        "https://www.youtube.com/watch?v=BkWLCrLapQo\n",
        "\n",
        "https://github.com/licong-lin/in-context-rl\n",
        "    \n",
        "###.\n",
        "### ALSO READ:\n",
        "\n",
        "Automatic hyperparameter optimization\n",
        "\n",
        "To optimize the parameters of C, we chose the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)\n",
        "\n",
        "evolution strategies (ES)\n",
        "\n",
        "###.\n",
        "Do androids dream of electric sheep?\n",
        "\n",
        "https://worldmodels.github.io/\n",
        "\n",
        "https://github.com/hardmaru/WorldModelsExperiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6H6OfSkdOZT",
        "outputId": "1b7cdb51-81e0-4543-cdeb-fc7d7b9d80c0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vizdoom in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vizdoom) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (0.29.1)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.3.0+cpu)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install vizdoom\n",
        "!pip install stable_baselines3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelType = \"PPO\"     # @param {type:\"string\"}\n",
        "modelName = \"DoomHome\"+modelType # @param {type:\"string\"}\n",
        "modelNew = True       # @param {type:\"boolean\"}\n",
        "modelTrain = True     # @param {type:\"boolean\"}\n",
        "\n",
        "print(modelName)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WfuZoeMbM8O",
        "outputId": "2482a70c-154d-4d75-ca05-f73bbb1138b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DoomHomePPO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U6fVbqDKccsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6c0b49-0e02-444d-cf04-21804b1ab38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "############### GYM ENV == VIZ DOOM ###########################################\n",
        "from vizdoom import DoomGame, GameVariable\n",
        "import numpy as np\n",
        "from gymnasium import Env\n",
        "from gymnasium.spaces import Discrete, Box\n",
        "import cv2\n",
        "\n",
        "class MyWayHomeGym(Env):\n",
        "    def __init__(self, scenario, render=True, number_of_actions=3):\n",
        "        self.game = DoomGame()\n",
        "        self.game.load_config(f\"{scenario}.cfg\")\n",
        "\n",
        "        # self.game.set_mode(Mode.SPECTATOR)  # spectator\n",
        "\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_X)\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_Y)\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_Z)\n",
        "\n",
        "        self.pos = None\n",
        "\n",
        "        self.game.set_window_visible(render)\n",
        "        self.game.init()\n",
        "\n",
        "        self.observation_space = Box(\n",
        "            low=0, high=255, shape=(100, 160, 1), dtype=np.uint8\n",
        "        )\n",
        "        self.number_of_actions = number_of_actions\n",
        "        self.action_space = Discrete(number_of_actions)\n",
        "        self.wins = 0\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        actions = np.identity(self.number_of_actions)\n",
        "        # tics need to be more than 1 to learn (tics = 4)\n",
        "        tics = 1\n",
        "        if modelTrain: tics = 4\n",
        "        reward = self.game.make_action(actions[action], tics = tics)\n",
        "        if modelTrain:\n",
        "            reward *= 100\n",
        "        #    if reward < 0: reward *= 10\n",
        "\n",
        "        dist = 0\n",
        "        movement_reward = 0\n",
        "        if self.game.get_state():\n",
        "\n",
        "            # MOVEMENT REWARD\n",
        "            if modelTrain:\n",
        "                _, pos_x, pos_y, pos_z = self.game.get_state().game_variables\n",
        "                pos = np.array([pos_x, pos_y, pos_z])\n",
        "                if self.pos is not None:\n",
        "                    dist = np.sqrt(np.sum((pos - self.pos) ** 2))\n",
        "                    movement_reward = dist * 0.005\n",
        "                    self.pos = pos\n",
        "\n",
        "            state = self.game.get_state().screen_buffer\n",
        "            state = self.grayscale(state)\n",
        "            info = self.game.get_state().game_variables[0]  # ammo\n",
        "        else:\n",
        "            state = np.zeros(self.observation_space.shape)\n",
        "            info = 0\n",
        "\n",
        "        info = {\"info\": info}\n",
        "        terminated = self.game.is_episode_finished()\n",
        "\n",
        "        truncated = (\n",
        "            self.game.is_player_dead()\n",
        "            or self.game.is_player_dead()\n",
        "            or self.game.is_player_dead()\n",
        "        )\n",
        "\n",
        "        if reward > 0:\n",
        "            self.wins = self.wins + 1\n",
        "            print(f\"-************win reward = {reward} wins = {self.wins} *************************\")\n",
        "        elif terminated: print(f\"-------------end reward = {reward} / sum = {reward+movement_reward} dist = {dist} movement_reward = {movement_reward} \")\n",
        "        #elif modelTrain: print(f\"-------------end reward = {reward} / sum = {reward+movement_reward} dist = {dist} movement_reward = {movement_reward} \")\n",
        "\n",
        "        if modelTrain: reward += movement_reward\n",
        "        return state, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "    def reset(self, seed=0):\n",
        "        self.game.new_episode()\n",
        "        state = self.game.get_state().screen_buffer\n",
        "\n",
        "        if self.game.get_state():\n",
        "            info = self.game.get_state().game_variables[0]  # ammo\n",
        "        else:\n",
        "            info = 0\n",
        "\n",
        "        return (self.grayscale(state), {\"ammo\": info})\n",
        "\n",
        "    def grayscale(self, observation):\n",
        "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
        "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
        "        state = np.reshape(resize, (100, 160, 1))\n",
        "        return state\n",
        "\n",
        "    def close(self):\n",
        "        self.game.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lEFdol6dc2rj"
      },
      "outputs": [],
      "source": [
        "\n",
        "##################### SB3 : CALLBACK ##########################################\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "class TrainAndLoggingCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq, verbose=1, name=\"?\"):\n",
        "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.name = name\n",
        "\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            model_path = '{}_{}'.format(self.name, self.n_calls)\n",
        "            self.model.save(model_path)\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MQ0IVRQndj1z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNwOTGgIc7UM",
        "outputId": "58195c0e-727d-484b-959a-5a31883bdbe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 525      |\n",
            "|    ep_rew_mean     | -21      |\n",
            "| time/              |          |\n",
            "|    fps             | 240      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 1 *************************\n",
            "-------------end reward = -0.01 / sum = -0.01 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 487          |\n",
            "|    ep_rew_mean          | -6.97        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 141          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 28           |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048999423 |\n",
            "|    clip_fraction        | 0.0159       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | -0.142       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | -0.0174      |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00271     |\n",
            "|    value_loss           | 0.0195       |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 500         |\n",
            "|    ep_rew_mean          | -11.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 122         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 50          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011444718 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.00186     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 1.52        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0031     |\n",
            "|    value_loss           | 35          |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 2 *************************\n",
            "-------------end reward = -0.01 / sum = -0.01 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 477          |\n",
            "|    ep_rew_mean          | -7.31        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 115          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042605456 |\n",
            "|    clip_fraction        | 0.0617       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | -3.04        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.14         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00759     |\n",
            "|    value_loss           | 0.643        |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 486          |\n",
            "|    ep_rew_mean          | -9.92        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 111          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 91           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0102373315 |\n",
            "|    clip_fraction        | 0.115        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 0.0412       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.36         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00552     |\n",
            "|    value_loss           | 26.5         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 491         |\n",
            "|    ep_rew_mean          | -11.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 112         |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007349722 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.568       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.538       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.00738    |\n",
            "|    value_loss           | 1.89        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 496         |\n",
            "|    ep_rew_mean          | -12.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 107         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 133         |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011721184 |\n",
            "|    clip_fraction        | 0.112       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | -0.318      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.232       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0133     |\n",
            "|    value_loss           | 0.867       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 500          |\n",
            "|    ep_rew_mean          | -13.7        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 106          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 153          |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052799108 |\n",
            "|    clip_fraction        | 0.0544       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 0.404        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.0593       |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.014       |\n",
            "|    value_loss           | 0.29         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 502         |\n",
            "|    ep_rew_mean          | -14.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 105         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 174         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016260248 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | -0.144      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0595     |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.034      |\n",
            "|    value_loss           | 0.101       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 505         |\n",
            "|    ep_rew_mean          | -15.2       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 104         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 195         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022326216 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1          |\n",
            "|    explained_variance   | 0.264       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.091      |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0518     |\n",
            "|    value_loss           | 0.0423      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 506        |\n",
            "|    ep_rew_mean          | -15.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 104        |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 215        |\n",
            "|    total_timesteps      | 22528      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02442642 |\n",
            "|    clip_fraction        | 0.198      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.987     |\n",
            "|    explained_variance   | -0.41      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0782    |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | -0.0533    |\n",
            "|    value_loss           | 0.0326     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.97 wins = 3 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 4 *************************\n",
            "-------------end reward = -0.01 / sum = -0.01 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 501        |\n",
            "|    ep_rew_mean          | -11.9      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 236        |\n",
            "|    total_timesteps      | 24576      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01913132 |\n",
            "|    clip_fraction        | 0.176      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.991     |\n",
            "|    explained_variance   | -0.24      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.079     |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | -0.0418    |\n",
            "|    value_loss           | 0.0302     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 5 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 494        |\n",
            "|    ep_rew_mean          | -10.3      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 13         |\n",
            "|    time_elapsed         | 257        |\n",
            "|    total_timesteps      | 26624      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.21676868 |\n",
            "|    clip_fraction        | 0.67       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.687     |\n",
            "|    explained_variance   | 0.0127     |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 10.7       |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | 0.0221     |\n",
            "|    value_loss           | 46.6       |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.97 wins = 6 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 489         |\n",
            "|    ep_rew_mean          | -9.21       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 277         |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019262275 |\n",
            "|    clip_fraction        | 0.207       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.75       |\n",
            "|    explained_variance   | 0.0795      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 4.26        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | 0.0032      |\n",
            "|    value_loss           | 26.4        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 491          |\n",
            "|    ep_rew_mean          | -9.97        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 102          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 298          |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0148273455 |\n",
            "|    clip_fraction        | 0.122        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.572       |\n",
            "|    explained_variance   | 0.655        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 1.82         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00183     |\n",
            "|    value_loss           | 29.4         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 7 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 8 *************************\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 488         |\n",
            "|    ep_rew_mean          | -7.57       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 318         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035790026 |\n",
            "|    clip_fraction        | 0.203       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.655      |\n",
            "|    explained_variance   | -0.577      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.205       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.00873    |\n",
            "|    value_loss           | 1.08        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 490        |\n",
            "|    ep_rew_mean          | -8.32      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 102        |\n",
            "|    iterations           | 17         |\n",
            "|    time_elapsed         | 339        |\n",
            "|    total_timesteps      | 34816      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08696714 |\n",
            "|    clip_fraction        | 0.348      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.569     |\n",
            "|    explained_variance   | 0.353      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 1.33       |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | 0.0101     |\n",
            "|    value_loss           | 24.9       |\n",
            "----------------------------------------\n",
            "-************win reward = 99.97 wins = 9 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 487         |\n",
            "|    ep_rew_mean          | -7.46       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 360         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020006368 |\n",
            "|    clip_fraction        | 0.0896      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.274      |\n",
            "|    explained_variance   | -0.126      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0275      |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.00511    |\n",
            "|    value_loss           | 0.216       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 10 *************************\n",
            "-------------end reward = -0.01 / sum = -0.01 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 483         |\n",
            "|    ep_rew_mean          | -6.83       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 381         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027308278 |\n",
            "|    clip_fraction        | 0.331       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.797      |\n",
            "|    explained_variance   | 0.365       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 1.92        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | 0.00566     |\n",
            "|    value_loss           | 12.7        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 11 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 480         |\n",
            "|    ep_rew_mean          | -6.26       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 101         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 402         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017584927 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.696      |\n",
            "|    explained_variance   | 0.492       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 6.14        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00527    |\n",
            "|    value_loss           | 19.9        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 482         |\n",
            "|    ep_rew_mean          | -6.92       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 101         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 422         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022045106 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.677      |\n",
            "|    explained_variance   | 0.336       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 46.5        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.00361    |\n",
            "|    value_loss           | 31.1        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 12 *************************\n",
            "-------------end reward = -0.01 / sum = -0.01 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 479        |\n",
            "|    ep_rew_mean          | -6.25      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 101        |\n",
            "|    iterations           | 22         |\n",
            "|    time_elapsed         | 443        |\n",
            "|    total_timesteps      | 45056      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06444945 |\n",
            "|    clip_fraction        | 0.3        |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.647     |\n",
            "|    explained_variance   | -0.312     |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.0848     |\n",
            "|    n_updates            | 210        |\n",
            "|    policy_gradient_loss | -0.0122    |\n",
            "|    value_loss           | 0.452      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 481       |\n",
            "|    ep_rew_mean          | -6.86     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 101       |\n",
            "|    iterations           | 23        |\n",
            "|    time_elapsed         | 464       |\n",
            "|    total_timesteps      | 47104     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0300533 |\n",
            "|    clip_fraction        | 0.272     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.745    |\n",
            "|    explained_variance   | 0.196     |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 0.0587    |\n",
            "|    n_updates            | 220       |\n",
            "|    policy_gradient_loss | 0.00667   |\n",
            "|    value_loss           | 8.91      |\n",
            "---------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 482         |\n",
            "|    ep_rew_mean          | -7.28       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 101         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 485         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028045852 |\n",
            "|    clip_fraction        | 0.22        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.671      |\n",
            "|    explained_variance   | -1.35       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0404     |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0326     |\n",
            "|    value_loss           | 0.0851      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 482         |\n",
            "|    ep_rew_mean          | -7.28       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 101         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 506         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022890903 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.676      |\n",
            "|    explained_variance   | -1.25       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0621     |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0331     |\n",
            "|    value_loss           | 0.0619      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 485         |\n",
            "|    ep_rew_mean          | -8.41       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 100         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 527         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034797505 |\n",
            "|    clip_fraction        | 0.218       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.663      |\n",
            "|    explained_variance   | -0.309      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0407     |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0294     |\n",
            "|    value_loss           | 0.0835      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 13 *************************\n",
            "-------------end reward = -0.01 / sum = -0.01 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 481         |\n",
            "|    ep_rew_mean          | -7.24       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 100         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 548         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027475225 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.683      |\n",
            "|    explained_variance   | 0.0516      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0624      |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0205     |\n",
            "|    value_loss           | 0.0778      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 486         |\n",
            "|    ep_rew_mean          | -8.44       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 100         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 569         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.049316794 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.436      |\n",
            "|    explained_variance   | 0.339       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0745      |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | 0.00602     |\n",
            "|    value_loss           | 8.04        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 486         |\n",
            "|    ep_rew_mean          | -8.44       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 100         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 590         |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036377706 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.557      |\n",
            "|    explained_variance   | 0.33        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0209     |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0276     |\n",
            "|    value_loss           | 0.128       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 486         |\n",
            "|    ep_rew_mean          | -8.44       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 100         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 612         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031826008 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.706      |\n",
            "|    explained_variance   | -1          |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.00649     |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0227     |\n",
            "|    value_loss           | 0.103       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 486        |\n",
            "|    ep_rew_mean          | -8.44      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 100        |\n",
            "|    iterations           | 31         |\n",
            "|    time_elapsed         | 633        |\n",
            "|    total_timesteps      | 63488      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02004996 |\n",
            "|    clip_fraction        | 0.185      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.796     |\n",
            "|    explained_variance   | 0.221      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0282    |\n",
            "|    n_updates            | 300        |\n",
            "|    policy_gradient_loss | -0.0331    |\n",
            "|    value_loss           | 0.0607     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 486         |\n",
            "|    ep_rew_mean          | -8.44       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 100         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 654         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018402105 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.822      |\n",
            "|    explained_variance   | 0.379       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0263     |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0315     |\n",
            "|    value_loss           | 0.0667      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 14 *************************\n",
            "-------------end reward = -0.01 / sum = -0.01 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 482         |\n",
            "|    ep_rew_mean          | -7.27       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 100         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 675         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025791004 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.715      |\n",
            "|    explained_variance   | -0.459      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.00809     |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0281     |\n",
            "|    value_loss           | 0.138       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.98 wins = 15 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 478        |\n",
            "|    ep_rew_mean          | -6.13      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 34         |\n",
            "|    time_elapsed         | 696        |\n",
            "|    total_timesteps      | 69632      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06854434 |\n",
            "|    clip_fraction        | 0.335      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.619     |\n",
            "|    explained_variance   | 0.594      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.0904     |\n",
            "|    n_updates            | 330        |\n",
            "|    policy_gradient_loss | 0.00962    |\n",
            "|    value_loss           | 11.9       |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 480        |\n",
            "|    ep_rew_mean          | -7.2       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 35         |\n",
            "|    time_elapsed         | 717        |\n",
            "|    total_timesteps      | 71680      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01053251 |\n",
            "|    clip_fraction        | 0.127      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.409     |\n",
            "|    explained_variance   | 0.667      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 1.32       |\n",
            "|    n_updates            | 340        |\n",
            "|    policy_gradient_loss | 0.00361    |\n",
            "|    value_loss           | 14.6       |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 482         |\n",
            "|    ep_rew_mean          | -8.27       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 739         |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017377831 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.441      |\n",
            "|    explained_variance   | -0.172      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.127       |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0183     |\n",
            "|    value_loss           | 0.347       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 487         |\n",
            "|    ep_rew_mean          | -9.46       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 760         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.048350118 |\n",
            "|    clip_fraction        | 0.326       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.592      |\n",
            "|    explained_variance   | 0.487       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0371     |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.034      |\n",
            "|    value_loss           | 0.127       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 491         |\n",
            "|    ep_rew_mean          | -10.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 780         |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024872057 |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.507      |\n",
            "|    explained_variance   | 0.265       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.107       |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.0119     |\n",
            "|    value_loss           | 0.191       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 491         |\n",
            "|    ep_rew_mean          | -10.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 802         |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029110884 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.762      |\n",
            "|    explained_variance   | 0.247       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0484     |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.0346     |\n",
            "|    value_loss           | 0.0953      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.97 wins = 16 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 490         |\n",
            "|    ep_rew_mean          | -11.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 823         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.044999063 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.564      |\n",
            "|    explained_variance   | 0.604       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.154       |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.0387     |\n",
            "|    value_loss           | 0.113       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.98 wins = 17 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 489         |\n",
            "|    ep_rew_mean          | -11.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 844         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026597107 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.583      |\n",
            "|    explained_variance   | 0.307       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0523      |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.00479    |\n",
            "|    value_loss           | 7.94        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 489        |\n",
            "|    ep_rew_mean          | -11.6      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 42         |\n",
            "|    time_elapsed         | 865        |\n",
            "|    total_timesteps      | 86016      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03128985 |\n",
            "|    clip_fraction        | 0.215      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.561     |\n",
            "|    explained_variance   | 0.708      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 2.11       |\n",
            "|    n_updates            | 410        |\n",
            "|    policy_gradient_loss | 0.0134     |\n",
            "|    value_loss           | 10.3       |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 493         |\n",
            "|    ep_rew_mean          | -12.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 886         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028642066 |\n",
            "|    clip_fraction        | 0.196       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.571      |\n",
            "|    explained_variance   | -0.634      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0783      |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    value_loss           | 0.485       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 493        |\n",
            "|    ep_rew_mean          | -12.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 44         |\n",
            "|    time_elapsed         | 908        |\n",
            "|    total_timesteps      | 90112      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03709069 |\n",
            "|    clip_fraction        | 0.257      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.423     |\n",
            "|    explained_variance   | 0.209      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.0874     |\n",
            "|    n_updates            | 430        |\n",
            "|    policy_gradient_loss | -0.0302    |\n",
            "|    value_loss           | 0.141      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 498         |\n",
            "|    ep_rew_mean          | -13.9       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 928         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.046706002 |\n",
            "|    clip_fraction        | 0.375       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.656      |\n",
            "|    explained_variance   | 0.33        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0331     |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0311     |\n",
            "|    value_loss           | 0.0887      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.98 wins = 18 *************************\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 496         |\n",
            "|    ep_rew_mean          | -12.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 950         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036985185 |\n",
            "|    clip_fraction        | 0.218       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.749      |\n",
            "|    explained_variance   | 0.247       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0589     |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.0288     |\n",
            "|    value_loss           | 0.0839      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 500          |\n",
            "|    ep_rew_mean          | -14          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 99           |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 971          |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0135696065 |\n",
            "|    clip_fraction        | 0.167        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.65        |\n",
            "|    explained_variance   | 0.404        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.49         |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | 0.00417      |\n",
            "|    value_loss           | 19           |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.97 wins = 19 *************************\n",
            "-************win reward = 99.97 wins = 20 *************************\n",
            "-************win reward = 99.96000000000001 wins = 21 *************************\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 489         |\n",
            "|    ep_rew_mean          | -10.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 992         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032287307 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.715      |\n",
            "|    explained_variance   | 0.433       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0442     |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.0332     |\n",
            "|    value_loss           | 0.0861      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 489         |\n",
            "|    ep_rew_mean          | -10.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 1013        |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060168933 |\n",
            "|    clip_fraction        | 0.309       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.651      |\n",
            "|    explained_variance   | 0.266       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 12.6        |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | 0.00476     |\n",
            "|    value_loss           | 71.1        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.98 wins = 22 *************************\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 488         |\n",
            "|    ep_rew_mean          | -10.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 1034        |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023267606 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.62       |\n",
            "|    explained_variance   | -2.74       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0106      |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.026      |\n",
            "|    value_loss           | 0.23        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 488         |\n",
            "|    ep_rew_mean          | -10.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 1056        |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021326944 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.564      |\n",
            "|    explained_variance   | 0.366       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.672       |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | 0.00536     |\n",
            "|    value_loss           | 9.27        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-************win reward = 99.96000000000001 wins = 23 *************************\n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.04 / sum = -0.04 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 483         |\n",
            "|    ep_rew_mean          | -9.32       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 1077        |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023361173 |\n",
            "|    clip_fraction        | 0.195       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.52       |\n",
            "|    explained_variance   | -4.03       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0284      |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.0217     |\n",
            "|    value_loss           | 0.114       |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###################### TRAIN == SB3 ###########################################\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "\n",
        "callback = TrainAndLoggingCallback(check_freq=10000, name=modelName)\n",
        "env = MyWayHomeGym(render=False, scenario=\"my_way_home\")\n",
        "\n",
        "if modelNew:\n",
        "    model = PPO(\n",
        "        \"CnnPolicy\",\n",
        "        env,\n",
        "        verbose=1,\n",
        "        seed=0,\n",
        "        learning_rate=0.0001,\n",
        "        n_steps=2048,\n",
        "    )\n",
        "else:\n",
        "    model = PPO.load(modelName, env=env)\n",
        "\n",
        "# TRAIN\n",
        "if modelTrain:\n",
        "    model.learn(\n",
        "        total_timesteps=200000,\n",
        "        callback=callback,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka09sdlAdAeN"
      },
      "outputs": [],
      "source": [
        "###################### TEST == SB3 ########################################\n",
        "model = PPO.load(modelName, env=env)\n",
        "env = env = MyWayHomeGym(render=True, scenario=\"my_way_home\")\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "env.close()\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f}\")\n",
        "print(f\"std_reward:{std_reward:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZGSRQLCdAlb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyOBL+2weQqfOOEqQde0CqsZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}