{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccasanoval/RLtests/blob/master/DoomV4b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1PChgxjcgnW"
      },
      "source": [
        "#DoomV4b\n",
        "\n",
        "RL  = Stable Baseline 3 : PPO\n",
        "\n",
        "ENV = Gymnasium + VizDoom\n",
        "\n",
        "URL = https://github.com/AKapich/Reinforcement_Learning_Doom\n",
        "\n",
        "URL = https://github.com/AKapich/Reinforcement_Learning_Doom/blob/main/MyWayHome/my_way_home_env.py\n",
        "\n",
        "URL = https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "\n",
        "###.\n",
        "### ALSO TEST : RLlib\n",
        "\n",
        "###.\n",
        "###Â ALSTO TRY: callbacks\n",
        "\n",
        "https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/4_callbacks_hyperparameter_tuning.ipynb#scrollTo=adsKMvDkRUn0\n",
        "\n",
        "###.\n",
        "### ALSO READ: Algorithm Distilation + In Context RL\n",
        "\n",
        "https://www.youtube.com/watch?v=BkWLCrLapQo\n",
        "\n",
        "https://github.com/licong-lin/in-context-rl\n",
        "    \n",
        "###.\n",
        "### ALSO READ:\n",
        "\n",
        "Automatic hyperparameter optimization\n",
        "\n",
        "To optimize the parameters of C, we chose the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)\n",
        "\n",
        "evolution strategies (ES)\n",
        "\n",
        "###.\n",
        "Do androids dream of electric sheep?\n",
        "\n",
        "https://worldmodels.github.io/\n",
        "\n",
        "https://github.com/hardmaru/WorldModelsExperiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6H6OfSkdOZT",
        "outputId": "8376b85e-ca79-4716-92a6-de805fc48c93",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vizdoom in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vizdoom) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (0.29.1)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3) (12.6.20)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install vizdoom\n",
        "!pip install stable_baselines3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelType = \"PPO\"     # @param {type:\"string\"}\n",
        "modelName = \"DoomHome\"+modelType # @param {type:\"string\"}\n",
        "modelNew = True       # @param {type:\"boolean\"}\n",
        "modelTrain = True     # @param {type:\"boolean\"}\n",
        "\n",
        "print(modelName)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WfuZoeMbM8O",
        "outputId": "f692bf89-e270-44c5-aaf2-afefac479e26"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DoomHomePPO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "U6fVbqDKccsL"
      },
      "outputs": [],
      "source": [
        "############### GYM ENV == VIZ DOOM ###########################################\n",
        "from vizdoom import DoomGame, GameVariable\n",
        "import numpy as np\n",
        "from gymnasium import Env\n",
        "from gymnasium.spaces import Discrete, Box\n",
        "import cv2\n",
        "\n",
        "class MyWayHomeGym(Env):\n",
        "    def __init__(self, scenario, render=True, number_of_actions=3):\n",
        "        self.game = DoomGame()\n",
        "        self.game.load_config(f\"{scenario}.cfg\")\n",
        "\n",
        "        # self.game.set_mode(Mode.SPECTATOR)  # spectator\n",
        "\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_X)\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_Y)\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_Z)\n",
        "\n",
        "        self.pos = None\n",
        "\n",
        "        self.game.set_window_visible(render)\n",
        "        self.game.init()\n",
        "\n",
        "        self.observation_space = Box(\n",
        "            low=0, high=255, shape=(100, 160, 1), dtype=np.uint8\n",
        "        )\n",
        "        self.number_of_actions = number_of_actions\n",
        "        self.action_space = Discrete(number_of_actions)\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        actions = np.identity(self.number_of_actions)\n",
        "        # tics need to be more than 1 to learn (tics = 4)\n",
        "        tics = 1\n",
        "        if modelTrain: tics = 4\n",
        "        reward = self.game.make_action(actions[action], tics = tics)\n",
        "        #if modelTrain:\n",
        "        #    reward *= 100\n",
        "        #    if reward < 0: reward *= 10\n",
        "\n",
        "        dist = 0\n",
        "        movement_reward = 0\n",
        "        if self.game.get_state():\n",
        "\n",
        "            # MOVEMENT REWARD\n",
        "            if modelTrain:\n",
        "                _, pos_x, pos_y, pos_z = self.game.get_state().game_variables\n",
        "                pos = np.array([pos_x, pos_y, pos_z])\n",
        "                if self.pos is not None:\n",
        "                    dist = np.sqrt(np.sum((pos - self.pos) ** 2))\n",
        "                    movement_reward = dist * 0.005\n",
        "                    self.pos = pos\n",
        "\n",
        "            state = self.game.get_state().screen_buffer\n",
        "            state = self.grayscale(state)\n",
        "            info = self.game.get_state().game_variables[0]  # ammo\n",
        "        else:\n",
        "            state = np.zeros(self.observation_space.shape)\n",
        "            info = 0\n",
        "\n",
        "        info = {\"info\": info}\n",
        "        terminated = self.game.is_episode_finished()\n",
        "\n",
        "        truncated = (\n",
        "            self.game.is_player_dead()\n",
        "            or self.game.is_player_dead()\n",
        "            or self.game.is_player_dead()\n",
        "        )\n",
        "\n",
        "        if reward > 0:\n",
        "            self.wins = self.wins + 1\n",
        "            print(f\"-------------win reward = {reward} wins = {self.wins} *************************\")\n",
        "        elif terminated: print(f\"-------------end reward = {reward} / sum = {reward+movement_reward} dist = {dist} movement_reward = {movement_reward} \")\n",
        "        #elif modelTrain: print(f\"-------------end reward = {reward} / sum = {reward+movement_reward} dist = {dist} movement_reward = {movement_reward} \")\n",
        "\n",
        "        if modelTrain: reward += movement_reward\n",
        "        return state, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "    def reset(self, seed=0):\n",
        "        self.game.new_episode()\n",
        "        state = self.game.get_state().screen_buffer\n",
        "\n",
        "        if self.game.get_state():\n",
        "            info = self.game.get_state().game_variables[0]  # ammo\n",
        "        else:\n",
        "            info = 0\n",
        "\n",
        "        return (self.grayscale(state), {\"ammo\": info})\n",
        "\n",
        "    def grayscale(self, observation):\n",
        "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
        "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
        "        state = np.reshape(resize, (100, 160, 1))\n",
        "        return state\n",
        "\n",
        "    def close(self):\n",
        "        self.game.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lEFdol6dc2rj"
      },
      "outputs": [],
      "source": [
        "\n",
        "##################### SB3 : CALLBACK ##########################################\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "class TrainAndLoggingCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq, verbose=1, name=\"?\"):\n",
        "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.name = name\n",
        "\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            model_path = '{}_{}'.format(self.name, self.n_calls)\n",
        "            self.model.save(model_path)\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MQ0IVRQndj1z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNwOTGgIc7UM",
        "outputId": "6672ee37-7e6a-4f3c-8d1a-01ff77c4bbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 411      |\n",
            "|    ep_rew_mean     | 7.84     |\n",
            "| time/              |          |\n",
            "|    fps             | 93       |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 21       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 468          |\n",
            "|    ep_rew_mean          | 6.31         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 124          |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048550116 |\n",
            "|    clip_fraction        | 0.044        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | -0.203       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | -0.00809     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00657     |\n",
            "|    value_loss           | 0.0418       |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 487         |\n",
            "|    ep_rew_mean          | 7.45        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 27          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 226         |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009359919 |\n",
            "|    clip_fraction        | 0.0544      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.188       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0171     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0183     |\n",
            "|    value_loss           | 0.0161      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 496         |\n",
            "|    ep_rew_mean          | 7.5         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 25          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 327         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021219023 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0.118       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0829     |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0454     |\n",
            "|    value_loss           | 0.0214      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 468        |\n",
            "|    ep_rew_mean          | 7.69       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 23         |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 428        |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02184049 |\n",
            "|    clip_fraction        | 0.211      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.05      |\n",
            "|    explained_variance   | 0.292      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.108     |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0526    |\n",
            "|    value_loss           | 0.0244     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------win reward = 0.9997\n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 448         |\n",
            "|    ep_rew_mean          | 8.19        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 23          |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 529         |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027254203 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.446       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0848     |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.055      |\n",
            "|    value_loss           | 0.0386      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 458        |\n",
            "|    ep_rew_mean          | 8.52       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 22         |\n",
            "|    iterations           | 7          |\n",
            "|    time_elapsed         | 630        |\n",
            "|    total_timesteps      | 14336      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03770781 |\n",
            "|    clip_fraction        | 0.312      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.963     |\n",
            "|    explained_variance   | 0.495      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0864    |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | -0.0631    |\n",
            "|    value_loss           | 0.0421     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 466         |\n",
            "|    ep_rew_mean          | 9.03        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 22          |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 735         |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030718423 |\n",
            "|    clip_fraction        | 0.286       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.98       |\n",
            "|    explained_variance   | 0.49        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.073      |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0577     |\n",
            "|    value_loss           | 0.0597      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 472         |\n",
            "|    ep_rew_mean          | 8.92        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 21          |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 841         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033542972 |\n",
            "|    clip_fraction        | 0.296       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.933      |\n",
            "|    explained_variance   | 0.495       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0827     |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.061      |\n",
            "|    value_loss           | 0.0511      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 468         |\n",
            "|    ep_rew_mean          | 8.98        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 21          |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 943         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032125056 |\n",
            "|    clip_fraction        | 0.326       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.925      |\n",
            "|    explained_variance   | 0.42        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0747     |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0496     |\n",
            "|    value_loss           | 0.0453      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 466        |\n",
            "|    ep_rew_mean          | 9.24       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 21         |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 1047       |\n",
            "|    total_timesteps      | 22528      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02480254 |\n",
            "|    clip_fraction        | 0.248      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.932     |\n",
            "|    explained_variance   | 0.587      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0429    |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | -0.0495    |\n",
            "|    value_loss           | 0.0708     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 462        |\n",
            "|    ep_rew_mean          | 9.72       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 21         |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 1150       |\n",
            "|    total_timesteps      | 24576      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03701496 |\n",
            "|    clip_fraction        | 0.323      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.875     |\n",
            "|    explained_variance   | 0.369      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0574    |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | -0.0635    |\n",
            "|    value_loss           | 0.0863     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 467         |\n",
            "|    ep_rew_mean          | 10.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 21          |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 1252        |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033703744 |\n",
            "|    clip_fraction        | 0.299       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.828      |\n",
            "|    explained_variance   | 0.246       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.089      |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.058      |\n",
            "|    value_loss           | 0.0933      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9998\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 465        |\n",
            "|    ep_rew_mean          | 10.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 21         |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 1354       |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03828074 |\n",
            "|    clip_fraction        | 0.314      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.807     |\n",
            "|    explained_variance   | 0.288      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0854    |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.0617    |\n",
            "|    value_loss           | 0.0751     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 463         |\n",
            "|    ep_rew_mean          | 10.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 21          |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 1457        |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.043161705 |\n",
            "|    clip_fraction        | 0.313       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.817      |\n",
            "|    explained_variance   | 0.319       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0929     |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0513     |\n",
            "|    value_loss           | 0.0646      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 459         |\n",
            "|    ep_rew_mean          | 10.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 1560        |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031256124 |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.826      |\n",
            "|    explained_variance   | 0.311       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0538     |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0536     |\n",
            "|    value_loss           | 0.0793      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9998\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 457         |\n",
            "|    ep_rew_mean          | 11.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 1663        |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035621718 |\n",
            "|    clip_fraction        | 0.312       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.802      |\n",
            "|    explained_variance   | 0.39        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0933     |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0666     |\n",
            "|    value_loss           | 0.0799      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 460        |\n",
            "|    ep_rew_mean          | 11.4       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 18         |\n",
            "|    time_elapsed         | 1766       |\n",
            "|    total_timesteps      | 36864      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04642711 |\n",
            "|    clip_fraction        | 0.332      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.773     |\n",
            "|    explained_variance   | 0.224      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0776    |\n",
            "|    n_updates            | 170        |\n",
            "|    policy_gradient_loss | -0.0666    |\n",
            "|    value_loss           | 0.0793     |\n",
            "----------------------------------------\n",
            "-------------win reward = 0.9996\n",
            "-------------win reward = 0.9998\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 452         |\n",
            "|    ep_rew_mean          | 11.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 1870        |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.045637622 |\n",
            "|    clip_fraction        | 0.32        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.775      |\n",
            "|    explained_variance   | 0.339       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0771     |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0625     |\n",
            "|    value_loss           | 0.0862      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 456         |\n",
            "|    ep_rew_mean          | 11.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 1974        |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.048781894 |\n",
            "|    clip_fraction        | 0.315       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.689      |\n",
            "|    explained_variance   | 0.509       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0702     |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.045      |\n",
            "|    value_loss           | 0.0683      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 459         |\n",
            "|    ep_rew_mean          | 11.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 2077        |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.049643174 |\n",
            "|    clip_fraction        | 0.353       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.767      |\n",
            "|    explained_variance   | 0.521       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0662     |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0631     |\n",
            "|    value_loss           | 0.0945      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 461         |\n",
            "|    ep_rew_mean          | 11.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 2179        |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060076438 |\n",
            "|    clip_fraction        | 0.304       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.703      |\n",
            "|    explained_variance   | 0.328       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0791     |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0578     |\n",
            "|    value_loss           | 0.121       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 463         |\n",
            "|    ep_rew_mean          | 12.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 2281        |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.057178833 |\n",
            "|    clip_fraction        | 0.302       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.69       |\n",
            "|    explained_variance   | 0.413       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0483     |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0533     |\n",
            "|    value_loss           | 0.1         |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 459        |\n",
            "|    ep_rew_mean          | 12.7       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 24         |\n",
            "|    time_elapsed         | 2382       |\n",
            "|    total_timesteps      | 49152      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07262094 |\n",
            "|    clip_fraction        | 0.333      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.673     |\n",
            "|    explained_variance   | 0.088      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0974    |\n",
            "|    n_updates            | 230        |\n",
            "|    policy_gradient_loss | -0.0655    |\n",
            "|    value_loss           | 0.121      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 457         |\n",
            "|    ep_rew_mean          | 13.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 2484        |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060918443 |\n",
            "|    clip_fraction        | 0.302       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.649      |\n",
            "|    explained_variance   | 0.155       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0596     |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0563     |\n",
            "|    value_loss           | 0.118       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 457        |\n",
            "|    ep_rew_mean          | 13.5       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 26         |\n",
            "|    time_elapsed         | 2586       |\n",
            "|    total_timesteps      | 53248      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06463757 |\n",
            "|    clip_fraction        | 0.318      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.646     |\n",
            "|    explained_variance   | -0.0574    |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0733    |\n",
            "|    n_updates            | 250        |\n",
            "|    policy_gradient_loss | -0.0564    |\n",
            "|    value_loss           | 0.0997     |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 459         |\n",
            "|    ep_rew_mean          | 13.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 2688        |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.061611217 |\n",
            "|    clip_fraction        | 0.314       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.628      |\n",
            "|    explained_variance   | 0.17        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0662     |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0551     |\n",
            "|    value_loss           | 0.0986      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 465         |\n",
            "|    ep_rew_mean          | 14.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 2790        |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.057083867 |\n",
            "|    clip_fraction        | 0.313       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.629      |\n",
            "|    explained_variance   | 0.338       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.082      |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0522     |\n",
            "|    value_loss           | 0.0982      |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 473       |\n",
            "|    ep_rew_mean          | 14.7      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 20        |\n",
            "|    iterations           | 29        |\n",
            "|    time_elapsed         | 2894      |\n",
            "|    total_timesteps      | 59392     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0562466 |\n",
            "|    clip_fraction        | 0.331     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.653    |\n",
            "|    explained_variance   | 0.455     |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | -0.0593   |\n",
            "|    n_updates            | 280       |\n",
            "|    policy_gradient_loss | -0.0548   |\n",
            "|    value_loss           | 0.106     |\n",
            "---------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 473        |\n",
            "|    ep_rew_mean          | 15.1       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 30         |\n",
            "|    time_elapsed         | 2999       |\n",
            "|    total_timesteps      | 61440      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05226239 |\n",
            "|    clip_fraction        | 0.284      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.624     |\n",
            "|    explained_variance   | 0.407      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0595    |\n",
            "|    n_updates            | 290        |\n",
            "|    policy_gradient_loss | -0.0519    |\n",
            "|    value_loss           | 0.134      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 473        |\n",
            "|    ep_rew_mean          | 15.5       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 31         |\n",
            "|    time_elapsed         | 3102       |\n",
            "|    total_timesteps      | 63488      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06381297 |\n",
            "|    clip_fraction        | 0.302      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.593     |\n",
            "|    explained_variance   | 0.33       |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0412    |\n",
            "|    n_updates            | 300        |\n",
            "|    policy_gradient_loss | -0.0509    |\n",
            "|    value_loss           | 0.124      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 463        |\n",
            "|    ep_rew_mean          | 15.9       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 32         |\n",
            "|    time_elapsed         | 3205       |\n",
            "|    total_timesteps      | 65536      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06198219 |\n",
            "|    clip_fraction        | 0.291      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.569     |\n",
            "|    explained_variance   | 0.187      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0724    |\n",
            "|    n_updates            | 310        |\n",
            "|    policy_gradient_loss | -0.0522    |\n",
            "|    value_loss           | 0.113      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 467         |\n",
            "|    ep_rew_mean          | 16.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 3307        |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.055861037 |\n",
            "|    clip_fraction        | 0.284       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.581      |\n",
            "|    explained_variance   | 0.162       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0456     |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0549     |\n",
            "|    value_loss           | 0.145       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 469        |\n",
            "|    ep_rew_mean          | 16.9       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 34         |\n",
            "|    time_elapsed         | 3411       |\n",
            "|    total_timesteps      | 69632      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04435651 |\n",
            "|    clip_fraction        | 0.271      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.609     |\n",
            "|    explained_variance   | 0.292      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0299    |\n",
            "|    n_updates            | 330        |\n",
            "|    policy_gradient_loss | -0.0505    |\n",
            "|    value_loss           | 0.177      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 474        |\n",
            "|    ep_rew_mean          | 17.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 35         |\n",
            "|    time_elapsed         | 3515       |\n",
            "|    total_timesteps      | 71680      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06608619 |\n",
            "|    clip_fraction        | 0.309      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.585     |\n",
            "|    explained_variance   | 0.173      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0655    |\n",
            "|    n_updates            | 340        |\n",
            "|    policy_gradient_loss | -0.0565    |\n",
            "|    value_loss           | 0.111      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 474        |\n",
            "|    ep_rew_mean          | 17.6       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 36         |\n",
            "|    time_elapsed         | 3618       |\n",
            "|    total_timesteps      | 73728      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06788453 |\n",
            "|    clip_fraction        | 0.3        |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.58      |\n",
            "|    explained_variance   | 0.272      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0663    |\n",
            "|    n_updates            | 350        |\n",
            "|    policy_gradient_loss | -0.0581    |\n",
            "|    value_loss           | 0.123      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9998\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 473        |\n",
            "|    ep_rew_mean          | 17.9       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 37         |\n",
            "|    time_elapsed         | 3721       |\n",
            "|    total_timesteps      | 75776      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05313295 |\n",
            "|    clip_fraction        | 0.288      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.602     |\n",
            "|    explained_variance   | 0.214      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0149    |\n",
            "|    n_updates            | 360        |\n",
            "|    policy_gradient_loss | -0.0543    |\n",
            "|    value_loss           | 0.139      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 474        |\n",
            "|    ep_rew_mean          | 18.2       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 38         |\n",
            "|    time_elapsed         | 3825       |\n",
            "|    total_timesteps      | 77824      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06405504 |\n",
            "|    clip_fraction        | 0.32       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.579     |\n",
            "|    explained_variance   | 0.172      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.067     |\n",
            "|    n_updates            | 370        |\n",
            "|    policy_gradient_loss | -0.0568    |\n",
            "|    value_loss           | 0.144      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 471         |\n",
            "|    ep_rew_mean          | 18.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 3929        |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060670324 |\n",
            "|    clip_fraction        | 0.3         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.579      |\n",
            "|    explained_variance   | 0.085       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.071      |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.057      |\n",
            "|    value_loss           | 0.132       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 479        |\n",
            "|    ep_rew_mean          | 18.8       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 40         |\n",
            "|    time_elapsed         | 4032       |\n",
            "|    total_timesteps      | 81920      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07514457 |\n",
            "|    clip_fraction        | 0.347      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.59      |\n",
            "|    explained_variance   | 0.152      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.067     |\n",
            "|    n_updates            | 390        |\n",
            "|    policy_gradient_loss | -0.059     |\n",
            "|    value_loss           | 0.135      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 475        |\n",
            "|    ep_rew_mean          | 18.9       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 41         |\n",
            "|    time_elapsed         | 4135       |\n",
            "|    total_timesteps      | 83968      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05550497 |\n",
            "|    clip_fraction        | 0.283      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.567     |\n",
            "|    explained_variance   | 0.316      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0284    |\n",
            "|    n_updates            | 400        |\n",
            "|    policy_gradient_loss | -0.0504    |\n",
            "|    value_loss           | 0.17       |\n",
            "----------------------------------------\n",
            "-------------win reward = 0.9998\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9997\n",
            "-------------win reward = 0.9997\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9998\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 471        |\n",
            "|    ep_rew_mean          | 19.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 42         |\n",
            "|    time_elapsed         | 4239       |\n",
            "|    total_timesteps      | 86016      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06701468 |\n",
            "|    clip_fraction        | 0.288      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.556     |\n",
            "|    explained_variance   | 0.151      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0579    |\n",
            "|    n_updates            | 410        |\n",
            "|    policy_gradient_loss | -0.0486    |\n",
            "|    value_loss           | 0.169      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 471        |\n",
            "|    ep_rew_mean          | 19.6       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 43         |\n",
            "|    time_elapsed         | 4342       |\n",
            "|    total_timesteps      | 88064      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06754486 |\n",
            "|    clip_fraction        | 0.306      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.51      |\n",
            "|    explained_variance   | 0.216      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0875    |\n",
            "|    n_updates            | 420        |\n",
            "|    policy_gradient_loss | -0.053     |\n",
            "|    value_loss           | 0.143      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 470         |\n",
            "|    ep_rew_mean          | 20          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 4444        |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.069330335 |\n",
            "|    clip_fraction        | 0.269       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.508      |\n",
            "|    explained_variance   | 0.186       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0542     |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.051      |\n",
            "|    value_loss           | 0.191       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 470         |\n",
            "|    ep_rew_mean          | 20.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 4549        |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.076470464 |\n",
            "|    clip_fraction        | 0.286       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.475      |\n",
            "|    explained_variance   | 0.151       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0424     |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0533     |\n",
            "|    value_loss           | 0.147       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 475        |\n",
            "|    ep_rew_mean          | 20.9       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 46         |\n",
            "|    time_elapsed         | 4653       |\n",
            "|    total_timesteps      | 94208      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.11441372 |\n",
            "|    clip_fraction        | 0.262      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.449     |\n",
            "|    explained_variance   | 0.213      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0507    |\n",
            "|    n_updates            | 450        |\n",
            "|    policy_gradient_loss | -0.0529    |\n",
            "|    value_loss           | 0.202      |\n",
            "----------------------------------------\n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 474         |\n",
            "|    ep_rew_mean          | 21.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 4757        |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.098946914 |\n",
            "|    clip_fraction        | 0.263       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.426      |\n",
            "|    explained_variance   | 0.164       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0572     |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0477     |\n",
            "|    value_loss           | 0.153       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 474        |\n",
            "|    ep_rew_mean          | 21.4       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 4861       |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06672578 |\n",
            "|    clip_fraction        | 0.227      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.428     |\n",
            "|    explained_variance   | 0.253      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.0144     |\n",
            "|    n_updates            | 470        |\n",
            "|    policy_gradient_loss | -0.0398    |\n",
            "|    value_loss           | 0.196      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9996\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 471        |\n",
            "|    ep_rew_mean          | 21.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 49         |\n",
            "|    time_elapsed         | 4965       |\n",
            "|    total_timesteps      | 100352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.08745006 |\n",
            "|    clip_fraction        | 0.264      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.413     |\n",
            "|    explained_variance   | -0.0278    |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0229    |\n",
            "|    n_updates            | 480        |\n",
            "|    policy_gradient_loss | -0.0431    |\n",
            "|    value_loss           | 0.172      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9998\n",
            "-------------win reward = 0.9998\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 467        |\n",
            "|    ep_rew_mean          | 21.6       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 50         |\n",
            "|    time_elapsed         | 5068       |\n",
            "|    total_timesteps      | 102400     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.10447584 |\n",
            "|    clip_fraction        | 0.315      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.405     |\n",
            "|    explained_variance   | 0.409      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0635    |\n",
            "|    n_updates            | 490        |\n",
            "|    policy_gradient_loss | -0.0518    |\n",
            "|    value_loss           | 0.148      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 467        |\n",
            "|    ep_rew_mean          | 21.7       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 51         |\n",
            "|    time_elapsed         | 5183       |\n",
            "|    total_timesteps      | 104448     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09809463 |\n",
            "|    clip_fraction        | 0.294      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.423     |\n",
            "|    explained_variance   | -0.0144    |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0517    |\n",
            "|    n_updates            | 500        |\n",
            "|    policy_gradient_loss | -0.0517    |\n",
            "|    value_loss           | 0.145      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 467        |\n",
            "|    ep_rew_mean          | 21.9       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 52         |\n",
            "|    time_elapsed         | 5286       |\n",
            "|    total_timesteps      | 106496     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.38215828 |\n",
            "|    clip_fraction        | 0.336      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.363     |\n",
            "|    explained_variance   | 0.107      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.101     |\n",
            "|    n_updates            | 510        |\n",
            "|    policy_gradient_loss | -0.0565    |\n",
            "|    value_loss           | 0.12       |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 467         |\n",
            "|    ep_rew_mean          | 22.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 5391        |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.077436835 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.377      |\n",
            "|    explained_variance   | 0.256       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0306     |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.0407     |\n",
            "|    value_loss           | 0.206       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 467         |\n",
            "|    ep_rew_mean          | 22.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 20          |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 5496        |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.095271535 |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.394      |\n",
            "|    explained_variance   | 0.127       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0442     |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.0415     |\n",
            "|    value_loss           | 0.192       |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 476        |\n",
            "|    ep_rew_mean          | 22.8       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 20         |\n",
            "|    iterations           | 55         |\n",
            "|    time_elapsed         | 5599       |\n",
            "|    total_timesteps      | 112640     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.11630423 |\n",
            "|    clip_fraction        | 0.293      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.422     |\n",
            "|    explained_variance   | 0.251      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0624    |\n",
            "|    n_updates            | 540        |\n",
            "|    policy_gradient_loss | -0.043     |\n",
            "|    value_loss           | 0.137      |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------win reward = 0.9998\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n"
          ]
        }
      ],
      "source": [
        "\n",
        "###################### TRAIN == SB3 ###########################################\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "\n",
        "callback = TrainAndLoggingCallback(check_freq=10000, name=modelName)\n",
        "env = MyWayHomeGym(render=False, scenario=\"my_way_home\")\n",
        "\n",
        "if modelNew:\n",
        "    model = PPO(\n",
        "        \"CnnPolicy\",\n",
        "        env,\n",
        "        verbose=1,\n",
        "        seed=0,\n",
        "        learning_rate=0.0001,\n",
        "        n_steps=2048,\n",
        "    )\n",
        "else:\n",
        "    model = PPO.load(modelName, env=env)\n",
        "\n",
        "# TRAIN\n",
        "if modelTrain:\n",
        "    model.learn(\n",
        "        total_timesteps=200000,\n",
        "        callback=callback,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka09sdlAdAeN"
      },
      "outputs": [],
      "source": [
        "###################### TEST == SB3 ########################################\n",
        "model = PPO.load(modelName, env=env)\n",
        "env = env = MyWayHomeGym(render=True, scenario=\"my_way_home\")\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "env.close()\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f}\")\n",
        "print(f\"std_reward:{std_reward:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZGSRQLCdAlb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyMptSmFmI36inDo6hPq2toU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}