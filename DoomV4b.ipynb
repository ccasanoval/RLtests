{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOa9FFfdMuF/eU3aZkE1mgm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccasanoval/RLtests/blob/master/DoomV4b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DoomV4b\n",
        "\n",
        "RL  = Stable Baseline 3 : PPO\n",
        "\n",
        "ENV = Gymnasium + VizDoom\n",
        "\n",
        "URL = https://github.com/AKapich/Reinforcement_Learning_Doom\n",
        "\n",
        "URL = https://stable-baselines3.readthedocs.io/en/master/"
      ],
      "metadata": {
        "id": "f1PChgxjcgnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vizdoom\n",
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6H6OfSkdOZT",
        "outputId": "5b57627e-3694-417e-bab8-ec6d922783da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vizdoom in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vizdoom) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (0.29.1)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3) (12.5.82)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "U6fVbqDKccsL"
      },
      "outputs": [],
      "source": [
        "############### GYM ENV == VIZ DOOM ###########################################\n",
        "from vizdoom import DoomGame\n",
        "from vizdoom import GameVariable\n",
        "import numpy as np\n",
        "from gymnasium import Env\n",
        "from gymnasium.spaces import Discrete, Box\n",
        "import cv2\n",
        "\n",
        "class MyWayHomeGym(Env):\n",
        "    def __init__(self, scenario, render=True, number_of_actions=3):\n",
        "        self.game = DoomGame()\n",
        "        self.game.load_config(f\"{scenario}.cfg\")\n",
        "\n",
        "        # self.game.set_mode(Mode.SPECTATOR)  # spectator\n",
        "\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_X)\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_Y)\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_Z)\n",
        "\n",
        "        self.pos = None\n",
        "\n",
        "        self.game.set_window_visible(render)\n",
        "        self.game.init()\n",
        "\n",
        "        self.pos_history_length = 200\n",
        "        self.position_history = [None] * self.pos_history_length\n",
        "        self.i = 0\n",
        "\n",
        "        # self.observation_space = Box(\n",
        "        #     low=0, high=255, shape=(100, 160, 320), dtype=np.uint8\n",
        "        # )\n",
        "        self.observation_space = Box(\n",
        "            low=0, high=255, shape=(100, 160, 1), dtype=np.uint8\n",
        "        )\n",
        "        self.number_of_actions = number_of_actions\n",
        "        self.action_space = Discrete(number_of_actions)\n",
        "\n",
        "    def step(self, action):\n",
        "        actions = np.identity(self.number_of_actions)\n",
        "        reward = self.game.make_action(actions[action], 4)\n",
        "\n",
        "        if self.game.get_state():\n",
        "            _, pos_x, pos_y, pos_z = self.game.get_state().game_variables\n",
        "            pos = np.array([pos_x, pos_y, pos_z])\n",
        "\n",
        "            cur_index = self.i % self.pos_history_length\n",
        "            self.position_history[cur_index] = pos\n",
        "\n",
        "            prev_pos = self.position_history[self.pos_history_length - cur_index - 1]\n",
        "\n",
        "            same_place_penalty = None\n",
        "            if np.array_equal(self.position_history[cur_index], prev_pos):\n",
        "                same_place_penalty = -1\n",
        "            else:\n",
        "                same_place_penalty = (\n",
        "                    -0.5\n",
        "                    / np.sqrt(\n",
        "                        np.sum((self.position_history[cur_index] - prev_pos) ** 2)\n",
        "                    )\n",
        "                    if prev_pos is not None\n",
        "                    else 0\n",
        "                )\n",
        "\n",
        "            same_place_penalty = max(-1, same_place_penalty)\n",
        "\n",
        "            reward += same_place_penalty\n",
        "\n",
        "            self.i += 1\n",
        "\n",
        "            movement_reward = 0\n",
        "            if self.pos is not None:\n",
        "                dist = np.sqrt(np.sum((pos - self.pos) ** 2))\n",
        "                movement_reward = dist * 0.005\n",
        "                reward += movement_reward\n",
        "\n",
        "            self.pos = pos\n",
        "\n",
        "            state = self.game.get_state().screen_buffer\n",
        "\n",
        "            green_reward = self.get_green_reward(np.moveaxis(state, 0, -1))\n",
        "\n",
        "            reward += green_reward\n",
        "            #print(movement_reward, green_reward, reward, same_place_penalty)\n",
        "\n",
        "            state = self.grayscale(state)\n",
        "            info = self.game.get_state().game_variables[0]  # ammo\n",
        "        else:\n",
        "            state = np.zeros(self.observation_space.shape)\n",
        "            info = 0\n",
        "\n",
        "        info = {\"info\": info}\n",
        "        terminated = self.game.is_episode_finished()\n",
        "\n",
        "        truncated = (\n",
        "            self.game.is_player_dead()\n",
        "            or self.game.is_player_dead()\n",
        "            or self.game.is_player_dead()\n",
        "        )\n",
        "\n",
        "        return state, reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, seed=0):\n",
        "        self.game.new_episode()\n",
        "        state = self.game.get_state().screen_buffer\n",
        "\n",
        "        if self.game.get_state():\n",
        "            info = self.game.get_state().game_variables[0]  # ammo\n",
        "        else:\n",
        "            info = 0\n",
        "\n",
        "        return (self.grayscale(state), {\"ammo\": info})\n",
        "\n",
        "    def grayscale(self, observation):\n",
        "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
        "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
        "        state = np.reshape(resize, (100, 160, 1))\n",
        "        return state\n",
        "\n",
        "    def get_green_reward(self, observation):\n",
        "        hsv = cv2.cvtColor(observation, cv2.COLOR_BGR2HSV)\n",
        "        mask_green = cv2.inRange(hsv, (36, 25, 25), (70, 255, 255))\n",
        "        imask_green = mask_green > 0\n",
        "        green = np.zeros_like(observation, np.uint8)\n",
        "        green[imask_green] = observation[imask_green]\n",
        "        #cv2.imwrite(\"green.jpg\", green)\n",
        "\n",
        "        green_px_count = np.count_nonzero(green)\n",
        "\n",
        "        # print(\"PX COUNT\", green_px_count)\n",
        "        if green_px_count > 800 and green_px_count < 3000:\n",
        "            #print(\"Vest visible!\")\n",
        "            return 0.3\n",
        "\n",
        "        pw = 10**6\n",
        "        return green_px_count / pw\n",
        "        # if green_px_count < 115000:\n",
        "        #     return green_px_count / pw\n",
        "        # else:\n",
        "        #     b = 115000 / pw\n",
        "        #     a = -1 / pw * 0.4\n",
        "        #     return a * (green_px_count - 115000) + b\n",
        "\n",
        "    def close(self):\n",
        "        self.game.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##################### SB3 : CALLBACK ##########################################\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "class TrainAndLoggingCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq, verbose=1, name=\"?\"):\n",
        "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.name = name\n",
        "\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            model_path = '{}_{}'.format(self.name, self.n_calls)\n",
        "            self.model.save(model_path)\n",
        "        return True\n",
        "\n"
      ],
      "metadata": {
        "id": "lEFdol6dc2rj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "modelType = \"PPO\"     # @param {type:\"string\"}\n",
        "modelName = \"DoomHome\"+modelType # @param {type:\"string\"}\n",
        "modelNew = False       # @param {type:\"boolean\"}\n",
        "modelTrain = True     # @param {type:\"boolean\"}\n",
        "\n",
        "print(modelName)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ0IVRQndj1z",
        "outputId": "1ae292f1-10db-4072-a2ae-f2d7ef04486a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DoomHomePPO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###################### TRAIN == SB3 ###########################################\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "\n",
        "callback = TrainAndLoggingCallback(check_freq=10000, name=modelName)\n",
        "env = MyWayHomeGym(render=False, scenario=\"my_way_home\")\n",
        "\n",
        "if modelNew:\n",
        "    model = PPO(\n",
        "        \"CnnPolicy\",\n",
        "        env,\n",
        "        verbose=1,\n",
        "        seed=0,\n",
        "        learning_rate=0.0001,\n",
        "        n_steps=2048,\n",
        "    )\n",
        "else:\n",
        "    model = PPO.load(modelName, env=env)\n",
        "\n",
        "# TRAIN\n",
        "if modelTrain:\n",
        "    model.learn(\n",
        "        total_timesteps=200000,\n",
        "        callback=callback,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNwOTGgIc7UM",
        "outputId": "d7fb4bf5-c193-47fe-e46a-930d00adbae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 525      |\n",
            "|    ep_rew_mean     | 53.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 84       |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 24       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 525        |\n",
            "|    ep_rew_mean          | 73.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 94         |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 43         |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02588726 |\n",
            "|    clip_fraction        | 0.212      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.634     |\n",
            "|    explained_variance   | 0.603      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.627      |\n",
            "|    n_updates            | 490        |\n",
            "|    policy_gradient_loss | -0.0308    |\n",
            "|    value_loss           | 2.64       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 525        |\n",
            "|    ep_rew_mean          | 75.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 96         |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 63         |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01910825 |\n",
            "|    clip_fraction        | 0.16       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.646     |\n",
            "|    explained_variance   | 0.0242     |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.839      |\n",
            "|    n_updates            | 500        |\n",
            "|    policy_gradient_loss | -0.0236    |\n",
            "|    value_loss           | 2.98       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 525         |\n",
            "|    ep_rew_mean          | 72          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 100         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 81          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018305564 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.654      |\n",
            "|    explained_variance   | 0.239       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.278       |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.0242     |\n",
            "|    value_loss           | 2.96        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 525         |\n",
            "|    ep_rew_mean          | 72.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 99          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027807191 |\n",
            "|    clip_fraction        | 0.181       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.627      |\n",
            "|    explained_variance   | 0.552       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.467       |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.0331     |\n",
            "|    value_loss           | 1.98        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 525        |\n",
            "|    ep_rew_mean          | 70.2       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 104        |\n",
            "|    iterations           | 6          |\n",
            "|    time_elapsed         | 117        |\n",
            "|    total_timesteps      | 12288      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02157145 |\n",
            "|    clip_fraction        | 0.176      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.644     |\n",
            "|    explained_variance   | 0.482      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.826      |\n",
            "|    n_updates            | 530        |\n",
            "|    policy_gradient_loss | -0.026     |\n",
            "|    value_loss           | 3.42       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 525         |\n",
            "|    ep_rew_mean          | 70.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 104         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 136         |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035411984 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.644      |\n",
            "|    explained_variance   | 0.66        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.663       |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.0353     |\n",
            "|    value_loss           | 2.25        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 525         |\n",
            "|    ep_rew_mean          | 69.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 105         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 154         |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016586808 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.69       |\n",
            "|    explained_variance   | 0.45        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 1.01        |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.0269     |\n",
            "|    value_loss           | 3           |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 525         |\n",
            "|    ep_rew_mean          | 69.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 106         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 172         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027786525 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.667      |\n",
            "|    explained_variance   | 0.594       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.34        |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.0312     |\n",
            "|    value_loss           | 2.62        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 525         |\n",
            "|    ep_rew_mean          | 68.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 107         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 190         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019230548 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.667      |\n",
            "|    explained_variance   | 0.455       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.396       |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.0288     |\n",
            "|    value_loss           | 3.15        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 518         |\n",
            "|    ep_rew_mean          | 68.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 107         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 209         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023483232 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.656      |\n",
            "|    explained_variance   | 0.54        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.484       |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.0358     |\n",
            "|    value_loss           | 2.59        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 518         |\n",
            "|    ep_rew_mean          | 68.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 108         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 226         |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027393196 |\n",
            "|    clip_fraction        | 0.243       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.655      |\n",
            "|    explained_variance   | 0.483       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.306       |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -0.0378     |\n",
            "|    value_loss           | 1.85        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 519        |\n",
            "|    ep_rew_mean          | 68.1       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 13         |\n",
            "|    time_elapsed         | 244        |\n",
            "|    total_timesteps      | 26624      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02137587 |\n",
            "|    clip_fraction        | 0.17       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.656     |\n",
            "|    explained_variance   | 0.576      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.386      |\n",
            "|    n_updates            | 600        |\n",
            "|    policy_gradient_loss | -0.0303    |\n",
            "|    value_loss           | 2.57       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 514         |\n",
            "|    ep_rew_mean          | 67.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 262         |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021669704 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.635      |\n",
            "|    explained_variance   | 0.519       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 2.11        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    value_loss           | 3.7         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 509        |\n",
            "|    ep_rew_mean          | 67.6       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 15         |\n",
            "|    time_elapsed         | 281        |\n",
            "|    total_timesteps      | 30720      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02400674 |\n",
            "|    clip_fraction        | 0.185      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.612     |\n",
            "|    explained_variance   | 0.599      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 0.822      |\n",
            "|    n_updates            | 620        |\n",
            "|    policy_gradient_loss | -0.0294    |\n",
            "|    value_loss           | 2.38       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 510        |\n",
            "|    ep_rew_mean          | 68.9       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 16         |\n",
            "|    time_elapsed         | 300        |\n",
            "|    total_timesteps      | 32768      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01697246 |\n",
            "|    clip_fraction        | 0.15       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.603     |\n",
            "|    explained_variance   | 0.153      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 1.31       |\n",
            "|    n_updates            | 630        |\n",
            "|    policy_gradient_loss | -0.025     |\n",
            "|    value_loss           | 3.93       |\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################### TEST == SB3 ########################################\n",
        "model = PPO.load(modelName, env=env)\n",
        "env = env = MyWayHomeGym(render=True, scenario=\"my_way_home\")\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "env.close()\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f}\")\n",
        "print(f\"std_reward:{std_reward:.2f}\")\n"
      ],
      "metadata": {
        "id": "ka09sdlAdAeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JZGSRQLCdAlb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}