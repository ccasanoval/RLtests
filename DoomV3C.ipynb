{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccasanoval/RLtests/blob/master/DoomV3C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Z7JaNoj2gb"
      },
      "source": [
        "RL = PyTorch + ENV = VizDoom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4iXxe2AGjsWh",
        "outputId": "b58cfbff-6a23-4378-a66b-c5133fc39e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vizdoom\n",
            "  Downloading vizdoom-1.2.3-cp310-cp310-manylinux_2_28_x86_64.whl (28.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/28.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vizdoom) (1.25.2)\n",
            "Collecting gymnasium>=0.28.0 (from vizdoom)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->vizdoom)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, vizdoom\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 vizdoom-1.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install vizdoom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EoE64V-ckBCK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import itertools as it\n",
        "import os\n",
        "import random\n",
        "from collections import deque\n",
        "from time import sleep, time\n",
        "\n",
        "import numpy as np\n",
        "import skimage.transform\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "\n",
        "import vizdoom as vzd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j8olE9EHkNE_"
      },
      "outputs": [],
      "source": [
        "# Q-learning settings\n",
        "learning_rate = 0.00025         # @param {type:\"number\"}\n",
        "discount_factor = 0.99          # @param {type:\"number\"}\n",
        "train_epochs = 20               # @param {type:\"integer\"}\n",
        "learning_steps_per_epoch = 7000 # @param {type:\"integer\"} #original=2.000\n",
        "replay_memory_size = 20000      # @param {type:\"integer\"} #original=10.000\n",
        "\n",
        "# NN learning settings\n",
        "batch_size = 128                # @param {type:\"integer\"} #original=64\n",
        "\n",
        "# Training regime\n",
        "test_episodes_per_epoch = 100\n",
        "\n",
        "# Other parameters\n",
        "frame_repeat = 10             # @param {type:\"integer\"} #original=12\n",
        "episodes_to_watch = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hW1Pt9hkFJCp"
      },
      "outputs": [],
      "source": [
        "#resolution = (30, 45)#original\n",
        "\n",
        "#[64, 16, 20, 25]    100x120\n",
        "#[64, 16, 10, 17]    60x90\n",
        "#[64, 16, 2, 6]      30x45\n",
        "#[64, 16, 7, 7]      50x50\n",
        "cy = 48\n",
        "cx = 64\n",
        "#res_xxx = 16 * 2 * 6   #--------------original [64, 16, 7, 11] = 30x45\n",
        "res_xxx = 16 * 7 * 11  #--------------#64, 16, 7, 11] = 48x64\n",
        "res_yyy = int(res_xxx/2)\n",
        "resolution = (cy, cx)\n",
        "\n",
        "\n",
        "model_savefile = \"./model-doom.pth\" # @param {type:\"string\"}\n",
        "save_model = True       # @param {type:\"boolean\"}\n",
        "load_model = False      # @param {type:\"boolean\"}\n",
        "skip_learning = False   # @param {type:\"boolean\"}\n",
        "\n",
        "# Configuration file path\n",
        "#https://github.com/Farama-Foundation/ViZDoom/blob/master/scenarios/basic.cfg\n",
        "#config_file_path = os.path.join(vzd.scenarios_path, \"simpler_basic.cfg\")\n",
        "#config_file_path = os.path.join(vzd.scenarios_path, \"rocket_basic.cfg\")\n",
        "#config_file_path = os.path.join(vzd.scenarios_path, \"basic.cfg\")\n",
        "config_file_path = os.path.join(vzd.scenarios_path, \"deadly_corridor.cfg\")\n",
        "#config_file_path = os.path.join(vzd.scenarios_path, \"deadthmatch.cfg\")\n",
        "#config_file_path = os.path.join(vzd.scenarios_path, \"defend_the_center.cfg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "89MzgDasE1WQ"
      },
      "outputs": [],
      "source": [
        "# Uses GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "41fnwuJtkTJr"
      },
      "outputs": [],
      "source": [
        "def preprocess(img):\n",
        "    \"\"\"Down samples image to resolution\"\"\"\n",
        "    img = skimage.transform.resize(img, resolution)\n",
        "    img = img.astype(np.float32)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zWafPRJWkUGP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_simple_game():\n",
        "    print(\"Initializing doom...\")\n",
        "    game = vzd.DoomGame()\n",
        "    game.load_config(config_file_path)\n",
        "    game.set_window_visible(False)\n",
        "    game.set_mode(vzd.Mode.PLAYER)\n",
        "    game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
        "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
        "    game.init()\n",
        "    print(\"Doom initialized.\")\n",
        "\n",
        "    return game\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r5oB1mlEkXhz"
      },
      "outputs": [],
      "source": [
        "def test(game, agent):\n",
        "    \"\"\"Runs a test_episodes_per_epoch episodes and prints the result\"\"\"\n",
        "    print(\"\\nTesting...\")\n",
        "    test_scores = []\n",
        "    for test_episode in trange(test_episodes_per_epoch, leave=False):\n",
        "        game.new_episode()\n",
        "        while not game.is_episode_finished():\n",
        "            state = preprocess(game.get_state().screen_buffer)\n",
        "            best_action_index = agent.get_action(state)\n",
        "\n",
        "            game.make_action(actions[best_action_index], frame_repeat)\n",
        "        r = game.get_total_reward()\n",
        "        test_scores.append(r)\n",
        "\n",
        "    test_scores = np.array(test_scores)\n",
        "    print(\n",
        "        \"Results: mean: {:.1f} +/- {:.1f},\".format(\n",
        "            test_scores.mean(), test_scores.std()\n",
        "        ),\n",
        "        \"min: %.1f\" % test_scores.min(),\n",
        "        \"max: %.1f\" % test_scores.max(),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XSMnJk4Ake6V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run(game, agent, actions, num_epochs, frame_repeat, steps_per_epoch=2000):\n",
        "    \"\"\"\n",
        "    Run num epochs of training episodes.\n",
        "    Skip frame_repeat number of frames after each action.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        game.new_episode()\n",
        "        train_scores = []\n",
        "        global_step = 0\n",
        "        print(f\"\\nEpoch #{epoch + 1}\")\n",
        "\n",
        "        for _ in trange(steps_per_epoch, leave=False):\n",
        "            state = preprocess(game.get_state().screen_buffer)\n",
        "            action = agent.get_action(state)\n",
        "            reward = game.make_action(actions[action], frame_repeat)\n",
        "            done = game.is_episode_finished()\n",
        "\n",
        "            if not done:\n",
        "                next_state = preprocess(game.get_state().screen_buffer)\n",
        "            else:\n",
        "                next_state = np.zeros((1, cy, cx)).astype(np.float32)\n",
        "\n",
        "            agent.append_memory(state, action, reward, next_state, done)\n",
        "\n",
        "            if global_step > agent.batch_size:\n",
        "                agent.train()\n",
        "\n",
        "            if done:\n",
        "                train_scores.append(game.get_total_reward())\n",
        "                game.new_episode()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        agent.update_target_net()\n",
        "        train_scores = np.array(train_scores)\n",
        "\n",
        "        print(\n",
        "            \"Results: mean: {:.1f} +/- {:.1f},\".format(\n",
        "                train_scores.mean(), train_scores.std()\n",
        "            ),\n",
        "            \"min: %.1f,\" % train_scores.min(),\n",
        "            \"max: %.1f,\" % train_scores.max(),\n",
        "        )\n",
        "\n",
        "        #test(game, agent)\n",
        "\n",
        "        if save_model:\n",
        "            print(\"Saving the network weights to:\", model_savefile)\n",
        "            torch.save(agent.q_net, model_savefile)\n",
        "        print(\"Total elapsed time: %.2f minutes\" % ((time() - start_time) / 60.0))\n",
        "\n",
        "    game.close()\n",
        "    return agent, game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6n5ts07skjOC"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DuelQNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This is Duel DQN architecture.\n",
        "    see https://arxiv.org/abs/1511.06581 for more information.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, available_actions_count):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, stride=2, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 8, kernel_size=3, stride=2, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 8, kernel_size=3, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.state_fc = nn.Sequential(nn.Linear(res_yyy, 64), nn.ReLU(), nn.Linear(64, 1))\n",
        "\n",
        "        self.advantage_fc = nn.Sequential(\n",
        "            nn.Linear(res_yyy, 64), nn.ReLU(), nn.Linear(64, available_actions_count)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        #print(\"A-----------------\", x.size())#[64, 16, 2, 6] => 16x2x6 = 192\n",
        "        x = x.view(-1, res_xxx)\n",
        "        x1 = x[:, :res_yyy]  # input for the net to calculate the state value\n",
        "        x2 = x[:, res_yyy:]  # relative advantage of actions in the state\n",
        "        state_value = self.state_fc(x1).reshape(-1, 1)\n",
        "        advantage_values = self.advantage_fc(x2)\n",
        "        x = state_value + (\n",
        "            advantage_values - advantage_values.mean(dim=1).reshape(-1, 1)\n",
        "        )\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A6GVLiKlkoVl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        action_size,\n",
        "        memory_size,\n",
        "        batch_size,\n",
        "        discount_factor,\n",
        "        lr,\n",
        "        load_model,\n",
        "        epsilon=1,\n",
        "        epsilon_decay=0.9996,\n",
        "        epsilon_min=0.1,\n",
        "    ):\n",
        "        self.action_size = action_size\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "        self.discount = discount_factor\n",
        "        self.lr = lr\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        if load_model:\n",
        "            print(\"Loading model from: \", model_savefile)\n",
        "            self.q_net = torch.load(model_savefile)\n",
        "            self.target_net = torch.load(model_savefile)\n",
        "            self.epsilon = self.epsilon_min\n",
        "\n",
        "        else:\n",
        "            print(\"Initializing new model\")\n",
        "            self.q_net = DuelQNet(action_size).to(DEVICE)\n",
        "            self.target_net = DuelQNet(action_size).to(DEVICE)\n",
        "\n",
        "        self.opt = optim.SGD(self.q_net.parameters(), lr=self.lr)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            return random.choice(range(self.action_size))\n",
        "        else:\n",
        "            state = np.expand_dims(state, axis=0)\n",
        "            state = torch.from_numpy(state).float().to(DEVICE)\n",
        "            action = torch.argmax(self.q_net(state)).item()\n",
        "            return action\n",
        "\n",
        "    def update_target_net(self):\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "    def append_memory(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def train(self):\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        batch = np.array(batch, dtype=object)\n",
        "\n",
        "        states = np.stack(batch[:, 0]).astype(float)\n",
        "        actions = batch[:, 1].astype(int)\n",
        "        rewards = batch[:, 2].astype(float)\n",
        "        next_states = np.stack(batch[:, 3]).astype(float)\n",
        "        dones = batch[:, 4].astype(bool)\n",
        "        not_dones = ~dones\n",
        "\n",
        "        row_idx = np.arange(self.batch_size)  # used for indexing the batch\n",
        "\n",
        "        # value of the next states with double q learning\n",
        "        # see https://arxiv.org/abs/1509.06461 for more information on double q learning\n",
        "        with torch.no_grad():\n",
        "            next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
        "            idx = row_idx, np.argmax(self.q_net(next_states).cpu().data.numpy(), 1)\n",
        "            next_state_values = self.target_net(next_states).cpu().data.numpy()[idx]\n",
        "            next_state_values = next_state_values[not_dones]\n",
        "\n",
        "        # this defines y = r + discount * max_a q(s', a)\n",
        "        q_targets = rewards.copy()\n",
        "        q_targets[not_dones] += self.discount * next_state_values\n",
        "        q_targets = torch.from_numpy(q_targets).float().to(DEVICE)\n",
        "\n",
        "        # this selects only the q values of the actions taken\n",
        "        idx = row_idx, actions\n",
        "        states = torch.from_numpy(states).float().to(DEVICE)\n",
        "        action_values = self.q_net(states)[idx].float().to(DEVICE)\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        td_error = self.criterion(q_targets, action_values)\n",
        "        td_error.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        else:\n",
        "            self.epsilon = self.epsilon_min\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olK6ZJc9kskW",
        "outputId": "70b881a1-5ea9-4eec-8e47-6da58a0d36c0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing doom...\n",
            "Doom initialized.\n",
            "Initializing new model\n",
            "\n",
            "Epoch #1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 122.4 +/- 153.4, min: -116.0, max: 799.3,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 10.73 minutes\n",
            "\n",
            "Epoch #2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 197.0 +/- 175.3, min: -116.0, max: 871.3,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 21.27 minutes\n",
            "\n",
            "Epoch #3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 182.4 +/- 184.9, min: -116.0, max: 2281.9,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 31.83 minutes\n",
            "\n",
            "Epoch #4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 179.6 +/- 165.1, min: -116.0, max: 894.2,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 42.15 minutes\n",
            "\n",
            "Epoch #5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 234.0 +/- 188.4, min: -116.0, max: 842.0,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 52.78 minutes\n",
            "\n",
            "Epoch #6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 214.0 +/- 172.2, min: -116.0, max: 780.3,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 63.20 minutes\n",
            "\n",
            "Epoch #7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 209.0 +/- 169.4, min: -116.0, max: 875.3,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 73.67 minutes\n",
            "\n",
            "Epoch #8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 173.7 +/- 164.3, min: -116.0, max: 858.6,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 84.04 minutes\n",
            "\n",
            "Epoch #9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 193.7 +/- 169.1, min: -116.0, max: 968.7,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 94.45 minutes\n",
            "\n",
            "Epoch #10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: mean: 199.7 +/- 176.6, min: -116.0, max: 1054.6,\n",
            "Saving the network weights to: ./model-doom.pth\n",
            "Total elapsed time: 104.91 minutes\n",
            "\n",
            "Epoch #11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 5434/7000 [08:12<03:01,  8.62it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize game and actions\n",
        "    game = create_simple_game()\n",
        "    n = game.get_available_buttons_size()\n",
        "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
        "\n",
        "    # Initialize our agent with the set parameters\n",
        "    agent = DQNAgent(\n",
        "        len(actions),\n",
        "        lr=learning_rate,\n",
        "        batch_size=batch_size,\n",
        "        memory_size=replay_memory_size,\n",
        "        discount_factor=discount_factor,\n",
        "        load_model=load_model,\n",
        "    )\n",
        "\n",
        "    # Run the training for the set number of epochs\n",
        "    if not skip_learning:\n",
        "        agent, game = run(\n",
        "            game,\n",
        "            agent,\n",
        "            actions,\n",
        "            num_epochs=train_epochs,\n",
        "            frame_repeat=frame_repeat,\n",
        "            steps_per_epoch=learning_steps_per_epoch,\n",
        "        )\n",
        "\n",
        "        print(\"======================================\")\n",
        "        print(\"Training finished. It's time to watch!\")\n",
        "\n",
        "    # Reinitialize the game with window visible\n",
        "    game.close()\n",
        "    game.set_window_visible(True)\n",
        "    game.set_mode(vzd.Mode.ASYNC_PLAYER)\n",
        "    game.init()\n",
        "\n",
        "    for _ in range(episodes_to_watch):\n",
        "        game.new_episode()\n",
        "        while not game.is_episode_finished():\n",
        "            state = preprocess(game.get_state().screen_buffer)\n",
        "            best_action_index = agent.get_action(state)\n",
        "\n",
        "            # Instead of make_action(a, frame_repeat) in order to make the animation smooth\n",
        "            game.set_action(actions[best_action_index])\n",
        "            for _ in range(frame_repeat):\n",
        "                game.advance_action()\n",
        "\n",
        "        # Sleep between episodes\n",
        "        sleep(1.0)\n",
        "        score = game.get_total_reward()\n",
        "        print(\"Total score: \", score)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyOOjkd9qDzMGHOQ9WArPSqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}