{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7mrN3BW0x69m9kRtcO1Fg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccasanoval/RLtests/blob/master/DoomV4c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DoomV4b\n",
        "\n",
        "RL  = Stable Baseline 3 : PPO\n",
        "\n",
        "ENV = Gymnasium + VizDoom\n",
        "\n",
        "URL = https://github.com/AKapich/Reinforcement_Learning_Doom\n",
        "\n",
        "URL = https://github.com/AKapich/Reinforcement_Learning_Doom/blob/main/MyWayHome/my_way_home_env.py\n",
        "\n",
        "URL = https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        ".\n",
        "\n",
        "Config VizDoom:\n",
        "\n",
        "http://www.cs.put.poznan.pl/visualdoomai/tutorial.html#config\n",
        "\n",
        "\n",
        "###.\n",
        "### ALSO TEST : RLlib\n",
        "\n",
        "###.\n",
        "###Â ALSTO TRY: callbacks\n",
        "\n",
        "https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/4_callbacks_hyperparameter_tuning.ipynb#scrollTo=adsKMvDkRUn0\n",
        "\n",
        "###.\n",
        "### ALSO READ: Algorithm Distilation + In Context RL\n",
        "\n",
        "https://www.youtube.com/watch?v=BkWLCrLapQo\n",
        "\n",
        "https://github.com/licong-lin/in-context-rl\n",
        "    \n",
        "###.\n",
        "### ALSO READ:\n",
        "\n",
        "Automatic hyperparameter optimization\n",
        "\n",
        "To optimize the parameters of C, we chose the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)\n",
        "\n",
        "evolution strategies (ES)\n",
        "\n",
        "###.\n",
        "Do androids dream of electric sheep?\n",
        "\n",
        "https://worldmodels.github.io/\n",
        "\n",
        "https://github.com/hardmaru/WorldModelsExperiments\n",
        "\n"
      ],
      "metadata": {
        "id": "-pprIbFX3zJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vizdoom\n",
        "!pip install stable_baselines3\n",
        "!pip install sb3-contrib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du4pBzLK4L8u",
        "outputId": "a87ad18e-f801-4e6f-d311-08ec0e32fc4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vizdoom in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vizdoom) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (0.29.1)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3) (12.6.20)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: sb3-contrib in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: stable-baselines3<3.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from sb3-contrib) (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0->sb3-contrib) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0->sb3-contrib) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0->sb3-contrib) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (12.6.20)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3<3.0,>=2.3.0->sb3-contrib) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "#save_path = f\"/content/gdrive/MyDrive/Data-PPO-LSTM/{game_name}/\"\n",
        "\n",
        "###################### PARAMETERS #############################################\n",
        "modelType = \"RPPO\"     # @param {type:\"string\"}\n",
        "modelName = \"DoomHome\"+modelType # @param {type:\"string\"}\n",
        "modelNew = False       # @param {type:\"boolean\"}\n",
        "modelTrain = True     # @param {type:\"boolean\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "caXwL8NH4MJi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WDSnPwA83wXe"
      },
      "outputs": [],
      "source": [
        "############### GYM ENV == VIZ DOOM ###########################################\n",
        "from vizdoom import DoomGame, GameVariable\n",
        "import numpy as np\n",
        "from gymnasium import Env\n",
        "from gymnasium.spaces import Discrete, Box\n",
        "import cv2\n",
        "\n",
        "class MyWayHomeGym(Env):\n",
        "    def __init__(self, scenario, render=True, number_of_actions=3):\n",
        "        self.game = DoomGame()\n",
        "        self.game.load_config(f\"{scenario}.cfg\")\n",
        "\n",
        "        # self.game.set_mode(Mode.SPECTATOR)  # spectator\n",
        "\n",
        "        self.game.set_episode_timeout(2100 - 1200)#21 - 6 = 15s\n",
        "\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_X)\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_Y)\n",
        "        self.game.add_available_game_variable(GameVariable.POSITION_Z)\n",
        "\n",
        "        self.pos = None\n",
        "\n",
        "        self.game.set_window_visible(render)\n",
        "        self.game.init()\n",
        "\n",
        "        self.observation_space = Box(\n",
        "            low=0, high=255, shape=(100, 160, 1), dtype=np.uint8\n",
        "        )\n",
        "        self.number_of_actions = number_of_actions\n",
        "        self.action_space = Discrete(number_of_actions)\n",
        "        self.wins = 0\n",
        "\n",
        "        # SAME PLACE PENALTY\n",
        "        self.pos_history_length = 200\n",
        "        self.position_history = [None] * self.pos_history_length\n",
        "        self.i = 0\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        actions = np.identity(self.number_of_actions)\n",
        "        # tics need to be more than 1 to learn (tics = 4)\n",
        "        tics = 1\n",
        "        if modelTrain: tics = 4\n",
        "        reward = self.game.make_action(actions[action], tics = tics)\n",
        "\n",
        "        dist = 0\n",
        "        movement_reward = 0\n",
        "        same_place_penalty = 0\n",
        "        if self.game.get_state():\n",
        "\n",
        "            # SAME POSITION PENALTY\n",
        "            _, pos_x, pos_y, pos_z = self.game.get_state().game_variables\n",
        "            pos = np.array([pos_x, pos_y, pos_z])\n",
        "            cur_index = self.i % self.pos_history_length\n",
        "            self.position_history[cur_index] = pos\n",
        "            prev_pos = self.position_history[self.pos_history_length - cur_index - 1]\n",
        "            if np.array_equal(self.position_history[cur_index], prev_pos):\n",
        "                same_place_penalty = -1\n",
        "            else:\n",
        "                same_place_penalty = (\n",
        "                    -0.5\n",
        "                    / np.sqrt(\n",
        "                        np.sum((self.position_history[cur_index] - prev_pos) ** 2)\n",
        "                    )\n",
        "                    if prev_pos is not None\n",
        "                    else 0\n",
        "                )\n",
        "            same_place_penalty = max(-1, same_place_penalty)\n",
        "            self.i += 1\n",
        "\n",
        "            # MOVEMENT REWARD\n",
        "            if modelTrain:\n",
        "                _, pos_x, pos_y, pos_z = self.game.get_state().game_variables\n",
        "                pos = np.array([pos_x, pos_y, pos_z])\n",
        "                if self.pos is not None:\n",
        "                    dist = np.sqrt(np.sum((pos - self.pos) ** 2))\n",
        "                    movement_reward = dist * 0.005\n",
        "                    self.pos = pos\n",
        "\n",
        "            state = self.game.get_state().screen_buffer\n",
        "            state = self.grayscale(state)\n",
        "            info = self.game.get_state().game_variables[0]  # ammo\n",
        "        else:\n",
        "            state = np.zeros(self.observation_space.shape)\n",
        "            info = 0\n",
        "\n",
        "        info = {\"info\": info}\n",
        "        terminated = self.game.is_episode_finished()\n",
        "\n",
        "        truncated = (\n",
        "            self.game.is_player_dead()\n",
        "            or self.game.is_player_dead()\n",
        "            or self.game.is_player_dead()\n",
        "        )\n",
        "\n",
        "        if reward > 0:\n",
        "            self.wins = self.wins + 1\n",
        "            print(f\"-************win reward = {reward} wins = {self.wins} *************************\")\n",
        "        elif terminated: print(f\"-------------end reward = {reward} / sum = {reward+movement_reward} dist = {dist} movement_reward = {movement_reward} \")\n",
        "        #elif modelTrain: print(f\"-------------end reward = {reward} / sum = {reward+movement_reward} dist = {dist} movement_reward = {movement_reward} \")\n",
        "\n",
        "        if modelTrain:\n",
        "            #reward *= 100\n",
        "            reward += movement_reward\n",
        "            reward += same_place_penalty\n",
        "        return state, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "    def reset(self, seed=0):\n",
        "        self.game.new_episode()\n",
        "        state = self.game.get_state().screen_buffer\n",
        "\n",
        "        if self.game.get_state():\n",
        "            info = self.game.get_state().game_variables[0]  # ammo\n",
        "        else:\n",
        "            info = 0\n",
        "\n",
        "        return (self.grayscale(state), {\"ammo\": info})\n",
        "\n",
        "    def grayscale(self, observation):\n",
        "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
        "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
        "        state = np.reshape(resize, (100, 160, 1))\n",
        "        return state\n",
        "\n",
        "    def close(self):\n",
        "        self.game.close()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################### SB3 : CALLBACK ##########################################\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "class TrainAndLoggingCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq, verbose=1, name=\"?\"):\n",
        "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.name = name\n",
        "\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            model_path = '{}_{}'.format(self.name, self.n_calls)\n",
        "            self.model.save(model_path)\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "I9h4dQrL4EcX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################### TRAIN == SB3 ###########################################\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from sb3_contrib import RecurrentPPO\n",
        "\n",
        "callback = TrainAndLoggingCallback(check_freq=10000, name=modelName)\n",
        "env = MyWayHomeGym(render=False, scenario=\"my_way_home\")\n",
        "\n",
        "if modelNew:\n",
        "    model = RecurrentPPO(\"CnnLstmPolicy\",\n",
        "        env,\n",
        "        verbose=1,\n",
        "        seed=0,\n",
        "        #learning_rate=0.0001,\n",
        "        #n_steps=2048,\n",
        "    )\n",
        "else:\n",
        "    model = RecurrentPPO.load(modelName, env=env)\n",
        "\n",
        "# TRAIN\n",
        "if modelTrain:\n",
        "    model.learn(\n",
        "        total_timesteps=400000,\n",
        "        callback=callback,\n",
        "    )\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl4Kj1Fb3_x0",
        "outputId": "3ebef073-9c92-4269-d9dc-0332d44f87f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: code expected at most 16 arguments, got 18\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: code expected at most 16 arguments, got 18\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "----------------------------\n",
            "| time/              |     |\n",
            "|    fps             | 71  |\n",
            "|    iterations      | 1   |\n",
            "|    time_elapsed    | 1   |\n",
            "|    total_timesteps | 128 |\n",
            "----------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | -57.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 11            |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 256           |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1348166e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.189        |\n",
            "|    explained_variance   | 0.00603       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 11.7          |\n",
            "|    n_updates            | 6250          |\n",
            "|    policy_gradient_loss | -2.97e-05     |\n",
            "|    value_loss           | 24.2          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | -57.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 8            |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 43           |\n",
            "|    total_timesteps      | 384          |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.192186e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.188       |\n",
            "|    explained_variance   | 0.00416      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.64         |\n",
            "|    n_updates            | 6260         |\n",
            "|    policy_gradient_loss | -9.03e-05    |\n",
            "|    value_loss           | 11.4         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | -75.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 9             |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 54            |\n",
            "|    total_timesteps      | 512           |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.0703453e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.18         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 22.2          |\n",
            "|    n_updates            | 6270          |\n",
            "|    policy_gradient_loss | 0.000432      |\n",
            "|    value_loss           | 46.1          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | -75.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 8             |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 74            |\n",
            "|    total_timesteps      | 640           |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7425045e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.178        |\n",
            "|    explained_variance   | 0.00704       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 21.1          |\n",
            "|    n_updates            | 6280          |\n",
            "|    policy_gradient_loss | -1.93e-05     |\n",
            "|    value_loss           | 43.7          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | -78           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 8             |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 86            |\n",
            "|    total_timesteps      | 768           |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.4121255e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.176        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 11.3          |\n",
            "|    n_updates            | 6290          |\n",
            "|    policy_gradient_loss | 7.26e-06      |\n",
            "|    value_loss           | 23.8          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | -78           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 113           |\n",
            "|    total_timesteps      | 896           |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.2365864e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.175        |\n",
            "|    explained_variance   | 0.00138       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 11.7          |\n",
            "|    n_updates            | 6300          |\n",
            "|    policy_gradient_loss | -0.000178     |\n",
            "|    value_loss           | 24            |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | -87.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 8            |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 122          |\n",
            "|    total_timesteps      | 1024         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.741348e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.17        |\n",
            "|    explained_variance   | 4.77e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 58.4         |\n",
            "|    n_updates            | 6310         |\n",
            "|    policy_gradient_loss | 0.000196     |\n",
            "|    value_loss           | 121          |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | -92.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 146           |\n",
            "|    total_timesteps      | 1152          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022944296 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.175        |\n",
            "|    explained_variance   | 0.00398       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.02          |\n",
            "|    n_updates            | 6320          |\n",
            "|    policy_gradient_loss | -0.000602     |\n",
            "|    value_loss           | 18.1          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | -92.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 165           |\n",
            "|    total_timesteps      | 1280          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014591124 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.212        |\n",
            "|    explained_variance   | 0.00785       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 43.4          |\n",
            "|    n_updates            | 6330          |\n",
            "|    policy_gradient_loss | 6.65e-05      |\n",
            "|    value_loss           | 89.8          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 225         |\n",
            "|    ep_rew_mean          | -95.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 175         |\n",
            "|    total_timesteps      | 1408        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 9.62778e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.242      |\n",
            "|    explained_variance   | 1.8e-05     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 33.6        |\n",
            "|    n_updates            | 6340        |\n",
            "|    policy_gradient_loss | 0.000152    |\n",
            "|    value_loss           | 71.3        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | -95.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 8             |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 191           |\n",
            "|    total_timesteps      | 1536          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.7971342e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.253        |\n",
            "|    explained_variance   | 0.0134        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 16            |\n",
            "|    n_updates            | 6350          |\n",
            "|    policy_gradient_loss | 0.000133      |\n",
            "|    value_loss           | 33.1          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | -87.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 8            |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 202          |\n",
            "|    total_timesteps      | 1664         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001364993 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.246       |\n",
            "|    explained_variance   | 0.0462       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.06         |\n",
            "|    n_updates            | 6360         |\n",
            "|    policy_gradient_loss | -0.000713    |\n",
            "|    value_loss           | 12.1         |\n",
            "------------------------------------------\n",
            "-************win reward = 0.9998 wins = 1 *************************\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | -83.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 8            |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 221          |\n",
            "|    total_timesteps      | 1792         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007633674 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.23        |\n",
            "|    explained_variance   | -0.0143      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.6         |\n",
            "|    n_updates            | 6370         |\n",
            "|    policy_gradient_loss | -0.00152     |\n",
            "|    value_loss           | 33.3         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 220           |\n",
            "|    ep_rew_mean          | -83.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 240           |\n",
            "|    total_timesteps      | 1920          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038969237 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.223        |\n",
            "|    explained_variance   | 0.00242       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 21.6          |\n",
            "|    n_updates            | 6380          |\n",
            "|    policy_gradient_loss | -0.000504     |\n",
            "|    value_loss           | 43.4          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 220           |\n",
            "|    ep_rew_mean          | -80.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 8             |\n",
            "|    iterations           | 16            |\n",
            "|    time_elapsed         | 249           |\n",
            "|    total_timesteps      | 2048          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00034126826 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.228        |\n",
            "|    explained_variance   | -0.00202      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.69          |\n",
            "|    n_updates            | 6390          |\n",
            "|    policy_gradient_loss | -0.000752     |\n",
            "|    value_loss           | 7.48          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 220           |\n",
            "|    ep_rew_mean          | -80.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 8             |\n",
            "|    iterations           | 17            |\n",
            "|    time_elapsed         | 265           |\n",
            "|    total_timesteps      | 2176          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.9421174e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.256        |\n",
            "|    explained_variance   | 0.000383      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 20.5          |\n",
            "|    n_updates            | 6400          |\n",
            "|    policy_gradient_loss | 0.000175      |\n",
            "|    value_loss           | 41.3          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 221           |\n",
            "|    ep_rew_mean          | -82.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 8             |\n",
            "|    iterations           | 18            |\n",
            "|    time_elapsed         | 273           |\n",
            "|    total_timesteps      | 2304          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5555644e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.271        |\n",
            "|    explained_variance   | 0.0574        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 35.8          |\n",
            "|    n_updates            | 6410          |\n",
            "|    policy_gradient_loss | -5.06e-05     |\n",
            "|    value_loss           | 73.9          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 221          |\n",
            "|    ep_rew_mean          | -82.7        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 8            |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 297          |\n",
            "|    total_timesteps      | 2432         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.037407e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.288       |\n",
            "|    explained_variance   | -0.00785     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 12.6         |\n",
            "|    n_updates            | 6420         |\n",
            "|    policy_gradient_loss | -0.00132     |\n",
            "|    value_loss           | 25.6         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 221          |\n",
            "|    ep_rew_mean          | -85.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 8            |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 309          |\n",
            "|    total_timesteps      | 2560         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005479455 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.315       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 64.1         |\n",
            "|    n_updates            | 6430         |\n",
            "|    policy_gradient_loss | -0.000523    |\n",
            "|    value_loss           | 130          |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 222           |\n",
            "|    ep_rew_mean          | -88.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 8             |\n",
            "|    iterations           | 21            |\n",
            "|    time_elapsed         | 331           |\n",
            "|    total_timesteps      | 2688          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00065027596 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.345        |\n",
            "|    explained_variance   | 0.00836       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 25.5          |\n",
            "|    n_updates            | 6440          |\n",
            "|    policy_gradient_loss | -0.000644     |\n",
            "|    value_loss           | 52.3          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | -88.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 22           |\n",
            "|    time_elapsed         | 354          |\n",
            "|    total_timesteps      | 2816         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.118013e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.339       |\n",
            "|    explained_variance   | 0.00749      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.5         |\n",
            "|    n_updates            | 6450         |\n",
            "|    policy_gradient_loss | -0.00111     |\n",
            "|    value_loss           | 48.3         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | -86.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 8            |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 365          |\n",
            "|    total_timesteps      | 2944         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.570116e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.373       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.25         |\n",
            "|    n_updates            | 6460         |\n",
            "|    policy_gradient_loss | -8.79e-05    |\n",
            "|    value_loss           | 6.48         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | -86.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 385          |\n",
            "|    total_timesteps      | 3072         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.293187e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.386       |\n",
            "|    explained_variance   | 0.00192      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 29           |\n",
            "|    n_updates            | 6470         |\n",
            "|    policy_gradient_loss | -0.000143    |\n",
            "|    value_loss           | 58           |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | -82.4        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 8            |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 397          |\n",
            "|    total_timesteps      | 3200         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.884391e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.404       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.7         |\n",
            "|    n_updates            | 6480         |\n",
            "|    policy_gradient_loss | -0.000169    |\n",
            "|    value_loss           | 21.9         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 222           |\n",
            "|    ep_rew_mean          | -82.4         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 26            |\n",
            "|    time_elapsed         | 419           |\n",
            "|    total_timesteps      | 3328          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00027785078 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.408        |\n",
            "|    explained_variance   | -0.0179       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 17.7          |\n",
            "|    n_updates            | 6490          |\n",
            "|    policy_gradient_loss | -0.0028       |\n",
            "|    value_loss           | 35.5          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 222         |\n",
            "|    ep_rew_mean          | -78.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 431         |\n",
            "|    total_timesteps      | 3456        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000697454 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.409      |\n",
            "|    explained_variance   | 8.94e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.29        |\n",
            "|    n_updates            | 6500        |\n",
            "|    policy_gradient_loss | -0.00163    |\n",
            "|    value_loss           | 8.7         |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | -77.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 456          |\n",
            "|    total_timesteps      | 3584         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015289919 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.376       |\n",
            "|    explained_variance   | 0.00615      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.4         |\n",
            "|    n_updates            | 6510         |\n",
            "|    policy_gradient_loss | -0.00254     |\n",
            "|    value_loss           | 46.8         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 222           |\n",
            "|    ep_rew_mean          | -77.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 29            |\n",
            "|    time_elapsed         | 479           |\n",
            "|    total_timesteps      | 3712          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016013673 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.356        |\n",
            "|    explained_variance   | -0.00543      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 16.2          |\n",
            "|    n_updates            | 6520          |\n",
            "|    policy_gradient_loss | -0.000667     |\n",
            "|    value_loss           | 33.4          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 223           |\n",
            "|    ep_rew_mean          | -77.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 30            |\n",
            "|    time_elapsed         | 489           |\n",
            "|    total_timesteps      | 3840          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00048350403 |\n",
            "|    clip_fraction        | 0.00234       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.371        |\n",
            "|    explained_variance   | -5.01e-06     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 18.4          |\n",
            "|    n_updates            | 6530          |\n",
            "|    policy_gradient_loss | -0.00113      |\n",
            "|    value_loss           | 37.1          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 223         |\n",
            "|    ep_rew_mean          | -77.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 504         |\n",
            "|    total_timesteps      | 3968        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012615496 |\n",
            "|    clip_fraction        | 0.0484      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.445      |\n",
            "|    explained_variance   | 0.00045     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 27.2        |\n",
            "|    n_updates            | 6540        |\n",
            "|    policy_gradient_loss | -0.00498    |\n",
            "|    value_loss           | 54.7        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | -79.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 515          |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006045983 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.498       |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.7         |\n",
            "|    n_updates            | 6550         |\n",
            "|    policy_gradient_loss | 0.00154      |\n",
            "|    value_loss           | 49           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | -79.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 538          |\n",
            "|    total_timesteps      | 4224         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002639545 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.502       |\n",
            "|    explained_variance   | -0.00104     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 14.2         |\n",
            "|    n_updates            | 6560         |\n",
            "|    policy_gradient_loss | -0.00151     |\n",
            "|    value_loss           | 28.4         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 223           |\n",
            "|    ep_rew_mean          | -78.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 34            |\n",
            "|    time_elapsed         | 551           |\n",
            "|    total_timesteps      | 4352          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.9932936e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.471        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 23.6          |\n",
            "|    n_updates            | 6570          |\n",
            "|    policy_gradient_loss | 0.00012       |\n",
            "|    value_loss           | 47.8          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-************win reward = 0.9996 wins = 2 *************************\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | -74.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 572          |\n",
            "|    total_timesteps      | 4480         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018590707 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.477       |\n",
            "|    explained_variance   | 0.00159      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.1         |\n",
            "|    n_updates            | 6580         |\n",
            "|    policy_gradient_loss | -0.00273     |\n",
            "|    value_loss           | 30.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | -74.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 601          |\n",
            "|    total_timesteps      | 4608         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019455142 |\n",
            "|    clip_fraction        | 0.00234      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.415       |\n",
            "|    explained_variance   | 0.0161       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 22.6         |\n",
            "|    n_updates            | 6590         |\n",
            "|    policy_gradient_loss | -0.00179     |\n",
            "|    value_loss           | 46.7         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 213         |\n",
            "|    ep_rew_mean          | -74         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 611         |\n",
            "|    total_timesteps      | 4736        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004548856 |\n",
            "|    clip_fraction        | 0.0117      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.376      |\n",
            "|    explained_variance   | -0.539      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.41        |\n",
            "|    n_updates            | 6600        |\n",
            "|    policy_gradient_loss | -0.00169    |\n",
            "|    value_loss           | 4.07        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 213        |\n",
            "|    ep_rew_mean          | -74        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 7          |\n",
            "|    iterations           | 38         |\n",
            "|    time_elapsed         | 627        |\n",
            "|    total_timesteps      | 4864       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00691074 |\n",
            "|    clip_fraction        | 0.0789     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.561     |\n",
            "|    explained_variance   | 0.102      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 33.2       |\n",
            "|    n_updates            | 6610       |\n",
            "|    policy_gradient_loss | -0.00848   |\n",
            "|    value_loss           | 70.7       |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -75.2        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 638          |\n",
            "|    total_timesteps      | 4992         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012403363 |\n",
            "|    clip_fraction        | 0.0219       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.622       |\n",
            "|    explained_variance   | 0.00683      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.3         |\n",
            "|    n_updates            | 6620         |\n",
            "|    policy_gradient_loss | -0.00168     |\n",
            "|    value_loss           | 37.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 214         |\n",
            "|    ep_rew_mean          | -75.2       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 655         |\n",
            "|    total_timesteps      | 5120        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006138243 |\n",
            "|    clip_fraction        | 0.0891      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.58       |\n",
            "|    explained_variance   | 0.0161      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18.2        |\n",
            "|    n_updates            | 6630        |\n",
            "|    policy_gradient_loss | -0.00693    |\n",
            "|    value_loss           | 38.6        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -73.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 664          |\n",
            "|    total_timesteps      | 5248         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020407308 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.52        |\n",
            "|    explained_variance   | 0.023        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.46         |\n",
            "|    n_updates            | 6640         |\n",
            "|    policy_gradient_loss | -0.00413     |\n",
            "|    value_loss           | 5.23         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -72.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 686          |\n",
            "|    total_timesteps      | 5376         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034441864 |\n",
            "|    clip_fraction        | 0.025        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.515       |\n",
            "|    explained_variance   | -0.0186      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.92         |\n",
            "|    n_updates            | 6650         |\n",
            "|    policy_gradient_loss | -0.00605     |\n",
            "|    value_loss           | 10.9         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -72.1         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 43            |\n",
            "|    time_elapsed         | 718           |\n",
            "|    total_timesteps      | 5504          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022976473 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.599        |\n",
            "|    explained_variance   | -0.0325       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 16.7          |\n",
            "|    n_updates            | 6660          |\n",
            "|    policy_gradient_loss | 5.91e-05      |\n",
            "|    value_loss           | 33.4          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -69.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 729          |\n",
            "|    total_timesteps      | 5632         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023202747 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.558       |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.96         |\n",
            "|    n_updates            | 6670         |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    value_loss           | 4.27         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -69.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 45            |\n",
            "|    time_elapsed         | 752           |\n",
            "|    total_timesteps      | 5760          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9942876e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.612        |\n",
            "|    explained_variance   | -0.00453      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.29          |\n",
            "|    n_updates            | 6680          |\n",
            "|    policy_gradient_loss | 0.000375      |\n",
            "|    value_loss           | 19.4          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -69.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 762          |\n",
            "|    total_timesteps      | 5888         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030026664 |\n",
            "|    clip_fraction        | 0.0234       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.643       |\n",
            "|    explained_variance   | -0.0174      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.9         |\n",
            "|    n_updates            | 6690         |\n",
            "|    policy_gradient_loss | -0.00255     |\n",
            "|    value_loss           | 31.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -69.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 780          |\n",
            "|    total_timesteps      | 6016         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0095808115 |\n",
            "|    clip_fraction        | 0.068        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.752       |\n",
            "|    explained_variance   | -0.00139     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.1         |\n",
            "|    n_updates            | 6700         |\n",
            "|    policy_gradient_loss | -0.00512     |\n",
            "|    value_loss           | 21.1         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | -69.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 789         |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004768713 |\n",
            "|    clip_fraction        | 0.032       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.833      |\n",
            "|    explained_variance   | 0.00453     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.4        |\n",
            "|    n_updates            | 6710        |\n",
            "|    policy_gradient_loss | -0.00325    |\n",
            "|    value_loss           | 36.8        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -67          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 813          |\n",
            "|    total_timesteps      | 6272         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019044564 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.834       |\n",
            "|    explained_variance   | 0.00352      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.63         |\n",
            "|    n_updates            | 6720         |\n",
            "|    policy_gradient_loss | -0.00682     |\n",
            "|    value_loss           | 7.75         |\n",
            "------------------------------------------\n",
            "-************win reward = 0.9997 wins = 3 *************************\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | -65.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 837          |\n",
            "|    total_timesteps      | 6400         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006885235 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.846       |\n",
            "|    explained_variance   | 0.00861      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.58         |\n",
            "|    n_updates            | 6730         |\n",
            "|    policy_gradient_loss | 0.000647     |\n",
            "|    value_loss           | 16.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 212         |\n",
            "|    ep_rew_mean          | -65.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 858         |\n",
            "|    total_timesteps      | 6528        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005768639 |\n",
            "|    clip_fraction        | 0.0133      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.856      |\n",
            "|    explained_variance   | -2.25e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.07        |\n",
            "|    n_updates            | 6740        |\n",
            "|    policy_gradient_loss | -0.00694    |\n",
            "|    value_loss           | 16.4        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 212         |\n",
            "|    ep_rew_mean          | -64.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 870         |\n",
            "|    total_timesteps      | 6656        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000750131 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.908      |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.594       |\n",
            "|    n_updates            | 6750        |\n",
            "|    policy_gradient_loss | 0.000969    |\n",
            "|    value_loss           | 1.68        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | -64.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 891          |\n",
            "|    total_timesteps      | 6784         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006944188 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.874       |\n",
            "|    explained_variance   | 0.0015       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.3         |\n",
            "|    n_updates            | 6760         |\n",
            "|    policy_gradient_loss | 0.000732     |\n",
            "|    value_loss           | 32.7         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | -62.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 907          |\n",
            "|    total_timesteps      | 6912         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014015324 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.872       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1          |\n",
            "|    n_updates            | 6770         |\n",
            "|    policy_gradient_loss | -0.00142     |\n",
            "|    value_loss           | 2.43         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 213           |\n",
            "|    ep_rew_mean          | -61.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 55            |\n",
            "|    time_elapsed         | 931           |\n",
            "|    total_timesteps      | 7040          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00063923467 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.858        |\n",
            "|    explained_variance   | -0.0107       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.43          |\n",
            "|    n_updates            | 6780          |\n",
            "|    policy_gradient_loss | -0.00184      |\n",
            "|    value_loss           | 9.92          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 213         |\n",
            "|    ep_rew_mean          | -61.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 953         |\n",
            "|    total_timesteps      | 7168        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007894826 |\n",
            "|    clip_fraction        | 0.0375      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.825      |\n",
            "|    explained_variance   | 0.0618      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.91        |\n",
            "|    n_updates            | 6790        |\n",
            "|    policy_gradient_loss | -0.00448    |\n",
            "|    value_loss           | 17.3        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -60.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 964          |\n",
            "|    total_timesteps      | 7296         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058287294 |\n",
            "|    clip_fraction        | 0.0563       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.748       |\n",
            "|    explained_variance   | 0.00105      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.431        |\n",
            "|    n_updates            | 6800         |\n",
            "|    policy_gradient_loss | -0.00391     |\n",
            "|    value_loss           | 1.3          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | -60.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 58            |\n",
            "|    time_elapsed         | 983           |\n",
            "|    total_timesteps      | 7424          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00036188914 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.747        |\n",
            "|    explained_variance   | -0.0113       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.99          |\n",
            "|    n_updates            | 6810          |\n",
            "|    policy_gradient_loss | -0.000888     |\n",
            "|    value_loss           | 16            |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -60.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 994          |\n",
            "|    total_timesteps      | 7552         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008260575 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.744       |\n",
            "|    explained_variance   | -1.1e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.721        |\n",
            "|    n_updates            | 6820         |\n",
            "|    policy_gradient_loss | -0.00302     |\n",
            "|    value_loss           | 1.62         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -60.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 1014         |\n",
            "|    total_timesteps      | 7680         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023436593 |\n",
            "|    clip_fraction        | 0.0164       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.775       |\n",
            "|    explained_variance   | 0.00569      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 30           |\n",
            "|    n_updates            | 6830         |\n",
            "|    policy_gradient_loss | -0.00551     |\n",
            "|    value_loss           | 60.6         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 214        |\n",
            "|    ep_rew_mean          | -60.2      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 7          |\n",
            "|    iterations           | 61         |\n",
            "|    time_elapsed         | 1025       |\n",
            "|    total_timesteps      | 7808       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00254494 |\n",
            "|    clip_fraction        | 0          |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.792     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 13.2       |\n",
            "|    n_updates            | 6840       |\n",
            "|    policy_gradient_loss | -0.00169   |\n",
            "|    value_loss           | 27.4       |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 214         |\n",
            "|    ep_rew_mean          | -58.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 1045        |\n",
            "|    total_timesteps      | 7936        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001027788 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.848      |\n",
            "|    explained_variance   | -0.0092     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.86        |\n",
            "|    n_updates            | 6850        |\n",
            "|    policy_gradient_loss | -0.00186    |\n",
            "|    value_loss           | 7.76        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | -58.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 63            |\n",
            "|    time_elapsed         | 1055          |\n",
            "|    total_timesteps      | 8064          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00063366257 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.869        |\n",
            "|    explained_variance   | 9.83e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.88          |\n",
            "|    n_updates            | 6860          |\n",
            "|    policy_gradient_loss | -0.000192     |\n",
            "|    value_loss           | 8.51          |\n",
            "-------------------------------------------\n",
            "-************win reward = 0.9996 wins = 4 *************************\n",
            "-------------end reward = -0.0001 / sum = -0.0001 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 213           |\n",
            "|    ep_rew_mean          | -57.2         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 64            |\n",
            "|    time_elapsed         | 1081          |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00090222014 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.874        |\n",
            "|    explained_variance   | -0.118        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.355         |\n",
            "|    n_updates            | 6870          |\n",
            "|    policy_gradient_loss | -0.00104      |\n",
            "|    value_loss           | 1.1           |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 213           |\n",
            "|    ep_rew_mean          | -55.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 65            |\n",
            "|    time_elapsed         | 1101          |\n",
            "|    total_timesteps      | 8320          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00057145115 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.877        |\n",
            "|    explained_variance   | -0.0097       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.15          |\n",
            "|    n_updates            | 6880          |\n",
            "|    policy_gradient_loss | -0.000671     |\n",
            "|    value_loss           | 7.12          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 213           |\n",
            "|    ep_rew_mean          | -55.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 66            |\n",
            "|    time_elapsed         | 1126          |\n",
            "|    total_timesteps      | 8448          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010265084 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.877        |\n",
            "|    explained_variance   | -0.0288       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.47          |\n",
            "|    n_updates            | 6890          |\n",
            "|    policy_gradient_loss | -0.00042      |\n",
            "|    value_loss           | 5.81          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -54.4        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 67           |\n",
            "|    time_elapsed         | 1138         |\n",
            "|    total_timesteps      | 8576         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028840788 |\n",
            "|    clip_fraction        | 0.0133       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.902       |\n",
            "|    explained_variance   | 0.000658     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.145        |\n",
            "|    n_updates            | 6900         |\n",
            "|    policy_gradient_loss | -0.00376     |\n",
            "|    value_loss           | 0.609        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 214         |\n",
            "|    ep_rew_mean          | -54.4       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 1163        |\n",
            "|    total_timesteps      | 8704        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001508574 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.929      |\n",
            "|    explained_variance   | -0.0269     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.74        |\n",
            "|    n_updates            | 6910        |\n",
            "|    policy_gradient_loss | -0.000168   |\n",
            "|    value_loss           | 1.96        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 214        |\n",
            "|    ep_rew_mean          | -53.3      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 7          |\n",
            "|    iterations           | 69         |\n",
            "|    time_elapsed         | 1175       |\n",
            "|    total_timesteps      | 8832       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04742937 |\n",
            "|    clip_fraction        | 0.253      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.985     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.332      |\n",
            "|    n_updates            | 6920       |\n",
            "|    policy_gradient_loss | -0.0181    |\n",
            "|    value_loss           | 0.738      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 214        |\n",
            "|    ep_rew_mean          | -53.3      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 7          |\n",
            "|    iterations           | 70         |\n",
            "|    time_elapsed         | 1194       |\n",
            "|    total_timesteps      | 8960       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00950821 |\n",
            "|    clip_fraction        | 0.0906     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.07      |\n",
            "|    explained_variance   | -0.00631   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.19       |\n",
            "|    n_updates            | 6930       |\n",
            "|    policy_gradient_loss | 0.00413    |\n",
            "|    value_loss           | 2.64       |\n",
            "----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -52.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 1204         |\n",
            "|    total_timesteps      | 9088         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011138218 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 1.09e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0592       |\n",
            "|    n_updates            | 6940         |\n",
            "|    policy_gradient_loss | -0.00197     |\n",
            "|    value_loss           | 0.318        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -52.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 1222         |\n",
            "|    total_timesteps      | 9216         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062876265 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 0.00133      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.34         |\n",
            "|    n_updates            | 6950         |\n",
            "|    policy_gradient_loss | -0.00769     |\n",
            "|    value_loss           | 10.7         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -51.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 73           |\n",
            "|    time_elapsed         | 1232         |\n",
            "|    total_timesteps      | 9344         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034597553 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 3.58e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.04         |\n",
            "|    n_updates            | 6960         |\n",
            "|    policy_gradient_loss | -0.00742     |\n",
            "|    value_loss           | 8.46         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 215         |\n",
            "|    ep_rew_mean          | -50.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 1258        |\n",
            "|    total_timesteps      | 9472        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.054695897 |\n",
            "|    clip_fraction        | 0.379       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.993      |\n",
            "|    explained_variance   | -0.00454    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.16        |\n",
            "|    n_updates            | 6970        |\n",
            "|    policy_gradient_loss | -0.0065     |\n",
            "|    value_loss           | 2.3         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 215         |\n",
            "|    ep_rew_mean          | -50.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 1279        |\n",
            "|    total_timesteps      | 9600        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008802172 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.841      |\n",
            "|    explained_variance   | -0.00364    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.1         |\n",
            "|    n_updates            | 6980        |\n",
            "|    policy_gradient_loss | 0.000279    |\n",
            "|    value_loss           | 4.34        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -50.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 76            |\n",
            "|    time_elapsed         | 1288          |\n",
            "|    total_timesteps      | 9728          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00067706266 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.782        |\n",
            "|    explained_variance   | -2.03e-06     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.06          |\n",
            "|    n_updates            | 6990          |\n",
            "|    policy_gradient_loss | 0.000613      |\n",
            "|    value_loss           | 14.2          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -50.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 77            |\n",
            "|    time_elapsed         | 1304          |\n",
            "|    total_timesteps      | 9856          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00031299656 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.763        |\n",
            "|    explained_variance   | 0.00185       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.6           |\n",
            "|    n_updates            | 7000          |\n",
            "|    policy_gradient_loss | -0.000385     |\n",
            "|    value_loss           | 13.7          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -50.1         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 78            |\n",
            "|    time_elapsed         | 1320          |\n",
            "|    total_timesteps      | 9984          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.7833574e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.751        |\n",
            "|    explained_variance   | -1.18e-05     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.33          |\n",
            "|    n_updates            | 7010          |\n",
            "|    policy_gradient_loss | -0.000361     |\n",
            "|    value_loss           | 0.676         |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -50.1         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 79            |\n",
            "|    time_elapsed         | 1343          |\n",
            "|    total_timesteps      | 10112         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00043358328 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.754        |\n",
            "|    explained_variance   | 0.00224       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.07          |\n",
            "|    n_updates            | 7020          |\n",
            "|    policy_gradient_loss | -0.000595     |\n",
            "|    value_loss           | 12.2          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -49.1         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 80            |\n",
            "|    time_elapsed         | 1356          |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00092039444 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.756        |\n",
            "|    explained_variance   | -1.19e-06     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.3           |\n",
            "|    n_updates            | 7030          |\n",
            "|    policy_gradient_loss | -0.0024       |\n",
            "|    value_loss           | 0.684         |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -48.4        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 1385         |\n",
            "|    total_timesteps      | 10368        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005109394 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.796       |\n",
            "|    explained_variance   | -0.00117     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.4          |\n",
            "|    n_updates            | 7040         |\n",
            "|    policy_gradient_loss | 0.000741     |\n",
            "|    value_loss           | 8.86         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -48.4         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 82            |\n",
            "|    time_elapsed         | 1409          |\n",
            "|    total_timesteps      | 10496         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00028450182 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.795        |\n",
            "|    explained_variance   | -0.00996      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.77          |\n",
            "|    n_updates            | 7050          |\n",
            "|    policy_gradient_loss | -0.000196     |\n",
            "|    value_loss           | 3.64          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -48.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 83            |\n",
            "|    time_elapsed         | 1421          |\n",
            "|    total_timesteps      | 10624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011251634 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.778        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 30.9          |\n",
            "|    n_updates            | 7060          |\n",
            "|    policy_gradient_loss | -5.23e-05     |\n",
            "|    value_loss           | 62.4          |\n",
            "-------------------------------------------\n",
            "-************win reward = 0.9998 wins = 5 *************************\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -48.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 84            |\n",
            "|    time_elapsed         | 1443          |\n",
            "|    total_timesteps      | 10752         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.3212665e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.774        |\n",
            "|    explained_variance   | -0.00411      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.17          |\n",
            "|    n_updates            | 7070          |\n",
            "|    policy_gradient_loss | -0.00034      |\n",
            "|    value_loss           | 4.46          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -48.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 1473         |\n",
            "|    total_timesteps      | 10880        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.295376e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.776       |\n",
            "|    explained_variance   | 0.000739     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 20.1         |\n",
            "|    n_updates            | 7080         |\n",
            "|    policy_gradient_loss | -0.000346    |\n",
            "|    value_loss           | 40.5         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -47.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 86            |\n",
            "|    time_elapsed         | 1485          |\n",
            "|    total_timesteps      | 11008         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00051031355 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.777        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.273         |\n",
            "|    n_updates            | 7090          |\n",
            "|    policy_gradient_loss | -0.000944     |\n",
            "|    value_loss           | 0.612         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 215         |\n",
            "|    ep_rew_mean          | -47.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 1505        |\n",
            "|    total_timesteps      | 11136       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009590615 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.819      |\n",
            "|    explained_variance   | 0.00231     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.1         |\n",
            "|    n_updates            | 7100        |\n",
            "|    policy_gradient_loss | -0.0101     |\n",
            "|    value_loss           | 4.23        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 215         |\n",
            "|    ep_rew_mean          | -47.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 1515        |\n",
            "|    total_timesteps      | 11264       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006881041 |\n",
            "|    clip_fraction        | 0.0133      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.753      |\n",
            "|    explained_variance   | -0.00223    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.283       |\n",
            "|    n_updates            | 7110        |\n",
            "|    policy_gradient_loss | -0.0021     |\n",
            "|    value_loss           | 0.708       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -47.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 1533         |\n",
            "|    total_timesteps      | 11392        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033357823 |\n",
            "|    clip_fraction        | 0.0164       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.683       |\n",
            "|    explained_variance   | 0.00553      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.26         |\n",
            "|    n_updates            | 7120         |\n",
            "|    policy_gradient_loss | -0.00234     |\n",
            "|    value_loss           | 6.57         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -46.2        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 1545         |\n",
            "|    total_timesteps      | 11520        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047165896 |\n",
            "|    clip_fraction        | 0.00391      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.645       |\n",
            "|    explained_variance   | -1.19e-06    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0592       |\n",
            "|    n_updates            | 7130         |\n",
            "|    policy_gradient_loss | -0.00178     |\n",
            "|    value_loss           | 0.307        |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -46.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 1567         |\n",
            "|    total_timesteps      | 11648        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001398311 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.572       |\n",
            "|    explained_variance   | 0.000864     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13.5         |\n",
            "|    n_updates            | 7140         |\n",
            "|    policy_gradient_loss | 0.00163      |\n",
            "|    value_loss           | 27.9         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -46.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 92            |\n",
            "|    time_elapsed         | 1593          |\n",
            "|    total_timesteps      | 11776         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3437588e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.566        |\n",
            "|    explained_variance   | -0.0198       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 32.9          |\n",
            "|    n_updates            | 7150          |\n",
            "|    policy_gradient_loss | -0.000181     |\n",
            "|    value_loss           | 67.2          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -46.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 1603         |\n",
            "|    total_timesteps      | 11904        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.861946e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.551       |\n",
            "|    explained_variance   | -0.000104    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.66         |\n",
            "|    n_updates            | 7160         |\n",
            "|    policy_gradient_loss | -0.000393    |\n",
            "|    value_loss           | 18           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -46.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 94           |\n",
            "|    time_elapsed         | 1625         |\n",
            "|    total_timesteps      | 12032        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.172232e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.55        |\n",
            "|    explained_variance   | 0.00416      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.91         |\n",
            "|    n_updates            | 7170         |\n",
            "|    policy_gradient_loss | -0.000131    |\n",
            "|    value_loss           | 6.07         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-************win reward = 0.9996 wins = 6 *************************\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 212           |\n",
            "|    ep_rew_mean          | -45.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 95            |\n",
            "|    time_elapsed         | 1638          |\n",
            "|    total_timesteps      | 12160         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013328157 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.571        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.511         |\n",
            "|    n_updates            | 7180          |\n",
            "|    policy_gradient_loss | -0.000359     |\n",
            "|    value_loss           | 1.01          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 212           |\n",
            "|    ep_rew_mean          | -45.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 96            |\n",
            "|    time_elapsed         | 1657          |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.5681686e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.583        |\n",
            "|    explained_variance   | 0.0117        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.77          |\n",
            "|    n_updates            | 7190          |\n",
            "|    policy_gradient_loss | -0.000488     |\n",
            "|    value_loss           | 5.94          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | -44.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 1668         |\n",
            "|    total_timesteps      | 12416        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017248541 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.588       |\n",
            "|    explained_variance   | 2.44e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.125        |\n",
            "|    n_updates            | 7200         |\n",
            "|    policy_gradient_loss | -0.00272     |\n",
            "|    value_loss           | 0.411        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 213         |\n",
            "|    ep_rew_mean          | -44.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 1687        |\n",
            "|    total_timesteps      | 12544       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005782047 |\n",
            "|    clip_fraction        | 0.0469      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.664      |\n",
            "|    explained_variance   | 0.00015     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.9        |\n",
            "|    n_updates            | 7210        |\n",
            "|    policy_gradient_loss | -0.00776    |\n",
            "|    value_loss           | 21.7        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | -44.4        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 99           |\n",
            "|    time_elapsed         | 1699         |\n",
            "|    total_timesteps      | 12672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029136576 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.722       |\n",
            "|    explained_variance   | -6.2e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.44         |\n",
            "|    n_updates            | 7220         |\n",
            "|    policy_gradient_loss | -0.00237     |\n",
            "|    value_loss           | 5.02         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 213           |\n",
            "|    ep_rew_mean          | -43.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 100           |\n",
            "|    time_elapsed         | 1724          |\n",
            "|    total_timesteps      | 12800         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014033867 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.757        |\n",
            "|    explained_variance   | -0.00998      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.1           |\n",
            "|    n_updates            | 7230          |\n",
            "|    policy_gradient_loss | 0.00211       |\n",
            "|    value_loss           | 2.22          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 213         |\n",
            "|    ep_rew_mean          | -43.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 101         |\n",
            "|    time_elapsed         | 1745        |\n",
            "|    total_timesteps      | 12928       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005404142 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.737      |\n",
            "|    explained_variance   | 0.00182     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.82        |\n",
            "|    n_updates            | 7240        |\n",
            "|    policy_gradient_loss | -0.00125    |\n",
            "|    value_loss           | 3.65        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 213         |\n",
            "|    ep_rew_mean          | -43.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 102         |\n",
            "|    time_elapsed         | 1755        |\n",
            "|    total_timesteps      | 13056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005954132 |\n",
            "|    clip_fraction        | 0.0195      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.684      |\n",
            "|    explained_variance   | 0.00447     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.291       |\n",
            "|    n_updates            | 7250        |\n",
            "|    policy_gradient_loss | -0.000868   |\n",
            "|    value_loss           | 0.693       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | -43.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 103          |\n",
            "|    time_elapsed         | 1775         |\n",
            "|    total_timesteps      | 13184        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014857017 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.642       |\n",
            "|    explained_variance   | -0.0127      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.36         |\n",
            "|    n_updates            | 7260         |\n",
            "|    policy_gradient_loss | -0.0025      |\n",
            "|    value_loss           | 3.14         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 213           |\n",
            "|    ep_rew_mean          | -42.9         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 104           |\n",
            "|    time_elapsed         | 1788          |\n",
            "|    total_timesteps      | 13312         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00045250263 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.608        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.38          |\n",
            "|    n_updates            | 7270          |\n",
            "|    policy_gradient_loss | -0.000173     |\n",
            "|    value_loss           | 14.8          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | -42.9        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 105          |\n",
            "|    time_elapsed         | 1811         |\n",
            "|    total_timesteps      | 13440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005964567 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.586       |\n",
            "|    explained_variance   | -0.0117      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.44         |\n",
            "|    n_updates            | 7280         |\n",
            "|    policy_gradient_loss | -0.00244     |\n",
            "|    value_loss           | 3.06         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | -42.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 106           |\n",
            "|    time_elapsed         | 1824          |\n",
            "|    total_timesteps      | 13568         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6804395e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.56         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.7           |\n",
            "|    n_updates            | 7290          |\n",
            "|    policy_gradient_loss | 0.0014        |\n",
            "|    value_loss           | 13.5          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | -43.2         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 107           |\n",
            "|    time_elapsed         | 1853          |\n",
            "|    total_timesteps      | 13696         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00019995216 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.56         |\n",
            "|    explained_variance   | -0.00243      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.49          |\n",
            "|    n_updates            | 7300          |\n",
            "|    policy_gradient_loss | -0.000711     |\n",
            "|    value_loss           | 3.02          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -43.2        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 108          |\n",
            "|    time_elapsed         | 1876         |\n",
            "|    total_timesteps      | 13824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003538155 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.536       |\n",
            "|    explained_variance   | 0.00436      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 39.9         |\n",
            "|    n_updates            | 7310         |\n",
            "|    policy_gradient_loss | -6.89e-05    |\n",
            "|    value_loss           | 80.5         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | -43.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 109           |\n",
            "|    time_elapsed         | 1889          |\n",
            "|    total_timesteps      | 13952         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.7324225e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.522        |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 40.8          |\n",
            "|    n_updates            | 7320          |\n",
            "|    policy_gradient_loss | 0.000456      |\n",
            "|    value_loss           | 83.8          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -43.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 110          |\n",
            "|    time_elapsed         | 1910         |\n",
            "|    total_timesteps      | 14080        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.591025e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.52        |\n",
            "|    explained_variance   | -0.0104      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.81         |\n",
            "|    n_updates            | 7330         |\n",
            "|    policy_gradient_loss | -0.000117    |\n",
            "|    value_loss           | 3.65         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | -43.4         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 111           |\n",
            "|    time_elapsed         | 1924          |\n",
            "|    total_timesteps      | 14208         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0253083e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.512        |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 17.5          |\n",
            "|    n_updates            | 7340          |\n",
            "|    policy_gradient_loss | -0.000178     |\n",
            "|    value_loss           | 35.3          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | -43.4         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 112           |\n",
            "|    time_elapsed         | 1948          |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012077857 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.504        |\n",
            "|    explained_variance   | -0.0102       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.65          |\n",
            "|    n_updates            | 7350          |\n",
            "|    policy_gradient_loss | -0.000687     |\n",
            "|    value_loss           | 3.41          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -43          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 113          |\n",
            "|    time_elapsed         | 1961         |\n",
            "|    total_timesteps      | 14464        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.534369e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.488       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.39         |\n",
            "|    n_updates            | 7360         |\n",
            "|    policy_gradient_loss | 0.000145     |\n",
            "|    value_loss           | 4.88         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | -42.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 114           |\n",
            "|    time_elapsed         | 1988          |\n",
            "|    total_timesteps      | 14592         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1651311e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.486        |\n",
            "|    explained_variance   | -0.015        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.36          |\n",
            "|    n_updates            | 7370          |\n",
            "|    policy_gradient_loss | -4.43e-05     |\n",
            "|    value_loss           | 2.8           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | -42.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 115          |\n",
            "|    time_elapsed         | 2012         |\n",
            "|    total_timesteps      | 14720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007635001 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.47        |\n",
            "|    explained_variance   | -0.00308     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.623        |\n",
            "|    n_updates            | 7380         |\n",
            "|    policy_gradient_loss | -0.00155     |\n",
            "|    value_loss           | 1.3          |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -42.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 116          |\n",
            "|    time_elapsed         | 2022         |\n",
            "|    total_timesteps      | 14848        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020002164 |\n",
            "|    clip_fraction        | 0.0109       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.41        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.4         |\n",
            "|    n_updates            | 7390         |\n",
            "|    policy_gradient_loss | -0.00253     |\n",
            "|    value_loss           | 20.8         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -42.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 117           |\n",
            "|    time_elapsed         | 2043          |\n",
            "|    total_timesteps      | 14976         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012928387 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.378        |\n",
            "|    explained_variance   | 0.00247       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 12.8          |\n",
            "|    n_updates            | 7400          |\n",
            "|    policy_gradient_loss | 0.000886      |\n",
            "|    value_loss           | 26.3          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -42.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 118          |\n",
            "|    time_elapsed         | 2054         |\n",
            "|    total_timesteps      | 15104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024453946 |\n",
            "|    clip_fraction        | 0.00547      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.353       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.15         |\n",
            "|    n_updates            | 7410         |\n",
            "|    policy_gradient_loss | -0.00148     |\n",
            "|    value_loss           | 2.3          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -42.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 119          |\n",
            "|    time_elapsed         | 2070         |\n",
            "|    total_timesteps      | 15232        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022892947 |\n",
            "|    clip_fraction        | 0.0141       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.285       |\n",
            "|    explained_variance   | 0.00228      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 20.2         |\n",
            "|    n_updates            | 7420         |\n",
            "|    policy_gradient_loss | -0.000139    |\n",
            "|    value_loss           | 41.4         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -43.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 120          |\n",
            "|    time_elapsed         | 2079         |\n",
            "|    total_timesteps      | 15360        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001791087 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.253       |\n",
            "|    explained_variance   | 1.49e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 41.2         |\n",
            "|    n_updates            | 7430         |\n",
            "|    policy_gradient_loss | 0.00134      |\n",
            "|    value_loss           | 85           |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -44.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 121          |\n",
            "|    time_elapsed         | 2101         |\n",
            "|    total_timesteps      | 15488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.516014e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.247       |\n",
            "|    explained_variance   | -0.00289     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.24         |\n",
            "|    n_updates            | 7440         |\n",
            "|    policy_gradient_loss | 4.73e-05     |\n",
            "|    value_loss           | 8.91         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -44.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 122           |\n",
            "|    time_elapsed         | 2129          |\n",
            "|    total_timesteps      | 15616         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.1224841e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.245        |\n",
            "|    explained_variance   | -0.00411      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 71.5          |\n",
            "|    n_updates            | 7450          |\n",
            "|    policy_gradient_loss | 2.64e-05      |\n",
            "|    value_loss           | 147           |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -45.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 123          |\n",
            "|    time_elapsed         | 2140         |\n",
            "|    total_timesteps      | 15744        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.852221e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.244       |\n",
            "|    explained_variance   | -0.00833     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 24           |\n",
            "|    n_updates            | 7460         |\n",
            "|    policy_gradient_loss | -2.84e-05    |\n",
            "|    value_loss           | 49.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -45.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 124          |\n",
            "|    time_elapsed         | 2157         |\n",
            "|    total_timesteps      | 15872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.891252e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.246       |\n",
            "|    explained_variance   | 0.00484      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 24.4         |\n",
            "|    n_updates            | 7470         |\n",
            "|    policy_gradient_loss | -0.000267    |\n",
            "|    value_loss           | 51.3         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -45.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 125           |\n",
            "|    time_elapsed         | 2168          |\n",
            "|    total_timesteps      | 16000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8211082e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.249        |\n",
            "|    explained_variance   | 0.0476        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.343         |\n",
            "|    n_updates            | 7480          |\n",
            "|    policy_gradient_loss | -0.000337     |\n",
            "|    value_loss           | 1.02          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -45.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 126           |\n",
            "|    time_elapsed         | 2186          |\n",
            "|    total_timesteps      | 16128         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3577752e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.253        |\n",
            "|    explained_variance   | 0.0364        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 32.5          |\n",
            "|    n_updates            | 7490          |\n",
            "|    policy_gradient_loss | -9.11e-05     |\n",
            "|    value_loss           | 66            |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -45.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 127          |\n",
            "|    time_elapsed         | 2198         |\n",
            "|    total_timesteps      | 16256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.940961e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.259       |\n",
            "|    explained_variance   | -9.78e-05    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.68         |\n",
            "|    n_updates            | 7500         |\n",
            "|    policy_gradient_loss | -0.000128    |\n",
            "|    value_loss           | 13.9         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 215        |\n",
            "|    ep_rew_mean          | -46.4      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 7          |\n",
            "|    iterations           | 128        |\n",
            "|    time_elapsed         | 2222       |\n",
            "|    total_timesteps      | 16384      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00174395 |\n",
            "|    clip_fraction        | 0.00469    |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.262     |\n",
            "|    explained_variance   | -0.0092    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.39       |\n",
            "|    n_updates            | 7510       |\n",
            "|    policy_gradient_loss | 5.25e-05   |\n",
            "|    value_loss           | 7.56       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -46.4        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 129          |\n",
            "|    time_elapsed         | 2253         |\n",
            "|    total_timesteps      | 16512        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.977213e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.26        |\n",
            "|    explained_variance   | 0.0186       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 61.2         |\n",
            "|    n_updates            | 7520         |\n",
            "|    policy_gradient_loss | 6.18e-05     |\n",
            "|    value_loss           | 124          |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | -47.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 130         |\n",
            "|    time_elapsed         | 2265        |\n",
            "|    total_timesteps      | 16640       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 7.20378e-07 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.285      |\n",
            "|    explained_variance   | 2.98e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18          |\n",
            "|    n_updates            | 7530        |\n",
            "|    policy_gradient_loss | -4.3e-05    |\n",
            "|    value_loss           | 37.1        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -47.1         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 131           |\n",
            "|    time_elapsed         | 2288          |\n",
            "|    total_timesteps      | 16768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.8754737e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.286        |\n",
            "|    explained_variance   | 0.0085        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 33.5          |\n",
            "|    n_updates            | 7540          |\n",
            "|    policy_gradient_loss | -2.48e-05     |\n",
            "|    value_loss           | 68.8          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -47.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 132           |\n",
            "|    time_elapsed         | 2300          |\n",
            "|    total_timesteps      | 16896         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.1420724e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.285        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 18.1          |\n",
            "|    n_updates            | 7550          |\n",
            "|    policy_gradient_loss | -3.4e-05      |\n",
            "|    value_loss           | 37.5          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -47.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 133          |\n",
            "|    time_elapsed         | 2318         |\n",
            "|    total_timesteps      | 17024        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.656178e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.287       |\n",
            "|    explained_variance   | 0.00475      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16           |\n",
            "|    n_updates            | 7560         |\n",
            "|    policy_gradient_loss | -0.000188    |\n",
            "|    value_loss           | 34.6         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | -47.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 134         |\n",
            "|    time_elapsed         | 2329        |\n",
            "|    total_timesteps      | 17152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002278687 |\n",
            "|    clip_fraction        | 0.0141      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.269      |\n",
            "|    explained_variance   | -0.0264     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.978       |\n",
            "|    n_updates            | 7570        |\n",
            "|    policy_gradient_loss | -0.00234    |\n",
            "|    value_loss           | 2.14        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | -46.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 135         |\n",
            "|    time_elapsed         | 2348        |\n",
            "|    total_timesteps      | 17280       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003096362 |\n",
            "|    clip_fraction        | 0.0195      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.283      |\n",
            "|    explained_variance   | -0.149      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.81        |\n",
            "|    n_updates            | 7580        |\n",
            "|    policy_gradient_loss | -0.00302    |\n",
            "|    value_loss           | 12.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | -46.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 136         |\n",
            "|    time_elapsed         | 2372        |\n",
            "|    total_timesteps      | 17408       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006432998 |\n",
            "|    clip_fraction        | 0.0203      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.284      |\n",
            "|    explained_variance   | -0.0598     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.02        |\n",
            "|    n_updates            | 7590        |\n",
            "|    policy_gradient_loss | -0.00456    |\n",
            "|    value_loss           | 6.29        |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -46.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 137           |\n",
            "|    time_elapsed         | 2383          |\n",
            "|    total_timesteps      | 17536         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00074387435 |\n",
            "|    clip_fraction        | 0.000781      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.194        |\n",
            "|    explained_variance   | -0.0025       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 11.1          |\n",
            "|    n_updates            | 7600          |\n",
            "|    policy_gradient_loss | -0.00128      |\n",
            "|    value_loss           | 23.4          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -46.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 138          |\n",
            "|    time_elapsed         | 2402         |\n",
            "|    total_timesteps      | 17664        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010437919 |\n",
            "|    clip_fraction        | 0.0141       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.228       |\n",
            "|    explained_variance   | -0.0131      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.87         |\n",
            "|    n_updates            | 7610         |\n",
            "|    policy_gradient_loss | -0.00202     |\n",
            "|    value_loss           | 8.05         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -46.7        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 139          |\n",
            "|    time_elapsed         | 2413         |\n",
            "|    total_timesteps      | 17792        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006984947 |\n",
            "|    clip_fraction        | 0.00234      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.271       |\n",
            "|    explained_variance   | -0.00399     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.965        |\n",
            "|    n_updates            | 7620         |\n",
            "|    policy_gradient_loss | -0.00103     |\n",
            "|    value_loss           | 1.97         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -46.7        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 140          |\n",
            "|    time_elapsed         | 2430         |\n",
            "|    total_timesteps      | 17920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003169817 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.295       |\n",
            "|    explained_variance   | -0.000681    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 19.6         |\n",
            "|    n_updates            | 7630         |\n",
            "|    policy_gradient_loss | -1.41e-06    |\n",
            "|    value_loss           | 41.5         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -47.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 141           |\n",
            "|    time_elapsed         | 2440          |\n",
            "|    total_timesteps      | 18048         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.7318637e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.278        |\n",
            "|    explained_variance   | -4.82e-05     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 28            |\n",
            "|    n_updates            | 7640          |\n",
            "|    policy_gradient_loss | -0.000298     |\n",
            "|    value_loss           | 59.9          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -48.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 142           |\n",
            "|    time_elapsed         | 2462          |\n",
            "|    total_timesteps      | 18176         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1235162e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.31         |\n",
            "|    explained_variance   | 0.00517       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.09          |\n",
            "|    n_updates            | 7650          |\n",
            "|    policy_gradient_loss | -0.000345     |\n",
            "|    value_loss           | 19.6          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -48.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 143           |\n",
            "|    time_elapsed         | 2475          |\n",
            "|    total_timesteps      | 18304         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00026835827 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.33         |\n",
            "|    explained_variance   | 3.22e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 47.4          |\n",
            "|    n_updates            | 7660          |\n",
            "|    policy_gradient_loss | 0.000197      |\n",
            "|    value_loss           | 104           |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -48.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 144           |\n",
            "|    time_elapsed         | 2496          |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00043328805 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.344        |\n",
            "|    explained_variance   | 0.00576       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.51          |\n",
            "|    n_updates            | 7670          |\n",
            "|    policy_gradient_loss | -0.00175      |\n",
            "|    value_loss           | 13.8          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -48.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 145           |\n",
            "|    time_elapsed         | 2520          |\n",
            "|    total_timesteps      | 18560         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00031448295 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.379        |\n",
            "|    explained_variance   | 0.0194        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 26.9          |\n",
            "|    n_updates            | 7680          |\n",
            "|    policy_gradient_loss | -0.00168      |\n",
            "|    value_loss           | 55.5          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | -48.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 146          |\n",
            "|    time_elapsed         | 2530         |\n",
            "|    total_timesteps      | 18688        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011220784 |\n",
            "|    clip_fraction        | 0.00391      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.342       |\n",
            "|    explained_variance   | 0.0656       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2          |\n",
            "|    n_updates            | 7690         |\n",
            "|    policy_gradient_loss | -0.00184     |\n",
            "|    value_loss           | 3.44         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 217           |\n",
            "|    ep_rew_mean          | -48.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 147           |\n",
            "|    time_elapsed         | 2547          |\n",
            "|    total_timesteps      | 18816         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00034336885 |\n",
            "|    clip_fraction        | 0.00156       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.332        |\n",
            "|    explained_variance   | 0.0224        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 14.6          |\n",
            "|    n_updates            | 7700          |\n",
            "|    policy_gradient_loss | -0.00222      |\n",
            "|    value_loss           | 29.7          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 217           |\n",
            "|    ep_rew_mean          | -49           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 148           |\n",
            "|    time_elapsed         | 2560          |\n",
            "|    total_timesteps      | 18944         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00050119776 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.371        |\n",
            "|    explained_variance   | 0.00052       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 17.5          |\n",
            "|    n_updates            | 7710          |\n",
            "|    policy_gradient_loss | -0.000991     |\n",
            "|    value_loss           | 37            |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 217         |\n",
            "|    ep_rew_mean          | -49         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 7           |\n",
            "|    iterations           | 149         |\n",
            "|    time_elapsed         | 2581        |\n",
            "|    total_timesteps      | 19072       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001948711 |\n",
            "|    clip_fraction        | 0.0156      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.319      |\n",
            "|    explained_variance   | 0.00288     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.4        |\n",
            "|    n_updates            | 7720        |\n",
            "|    policy_gradient_loss | -0.00121    |\n",
            "|    value_loss           | 22          |\n",
            "-----------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 217           |\n",
            "|    ep_rew_mean          | -49.2         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 150           |\n",
            "|    time_elapsed         | 2593          |\n",
            "|    total_timesteps      | 19200         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.5617805e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.271        |\n",
            "|    explained_variance   | 0.00017       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 18.3          |\n",
            "|    n_updates            | 7730          |\n",
            "|    policy_gradient_loss | -0.00022      |\n",
            "|    value_loss           | 38.7          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 217           |\n",
            "|    ep_rew_mean          | -48.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 151           |\n",
            "|    time_elapsed         | 2614          |\n",
            "|    total_timesteps      | 19328         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00048079202 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.268        |\n",
            "|    explained_variance   | 0.0298        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.18          |\n",
            "|    n_updates            | 7740          |\n",
            "|    policy_gradient_loss | -0.000395     |\n",
            "|    value_loss           | 9.83          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 217           |\n",
            "|    ep_rew_mean          | -48.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 152           |\n",
            "|    time_elapsed         | 2635          |\n",
            "|    total_timesteps      | 19456         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.3200244e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.268        |\n",
            "|    explained_variance   | -0.0267       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.36          |\n",
            "|    n_updates            | 7750          |\n",
            "|    policy_gradient_loss | -0.000268     |\n",
            "|    value_loss           | 7.61          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 217           |\n",
            "|    ep_rew_mean          | -49.1         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 153           |\n",
            "|    time_elapsed         | 2644          |\n",
            "|    total_timesteps      | 19584         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012040092 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.341        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 40.8          |\n",
            "|    n_updates            | 7760          |\n",
            "|    policy_gradient_loss | -0.00193      |\n",
            "|    value_loss           | 81.9          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 217           |\n",
            "|    ep_rew_mean          | -49.1         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 154           |\n",
            "|    time_elapsed         | 2664          |\n",
            "|    total_timesteps      | 19712         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 8.5177366e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.349        |\n",
            "|    explained_variance   | -0.00849      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.58          |\n",
            "|    n_updates            | 7770          |\n",
            "|    policy_gradient_loss | -0.00134      |\n",
            "|    value_loss           | 19.5          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-************win reward = 0.9996 wins = 7 *************************\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -49.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 155          |\n",
            "|    time_elapsed         | 2677         |\n",
            "|    total_timesteps      | 19840        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.315213e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.339       |\n",
            "|    explained_variance   | -0.000132    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 36.6         |\n",
            "|    n_updates            | 7780         |\n",
            "|    policy_gradient_loss | -0.000801    |\n",
            "|    value_loss           | 73.8         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -49.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 156           |\n",
            "|    time_elapsed         | 2698          |\n",
            "|    total_timesteps      | 19968         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.4122956e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.341        |\n",
            "|    explained_variance   | 0.00927       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 24.1          |\n",
            "|    n_updates            | 7790          |\n",
            "|    policy_gradient_loss | -0.000468     |\n",
            "|    value_loss           | 48.9          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -49.9         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 157           |\n",
            "|    time_elapsed         | 2709          |\n",
            "|    total_timesteps      | 20096         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00024854904 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.322        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 36.9          |\n",
            "|    n_updates            | 7800          |\n",
            "|    policy_gradient_loss | -0.00123      |\n",
            "|    value_loss           | 76.4          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -50.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 158           |\n",
            "|    time_elapsed         | 2736          |\n",
            "|    total_timesteps      | 20224         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9360486e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.311        |\n",
            "|    explained_variance   | -0.00846      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.64          |\n",
            "|    n_updates            | 7810          |\n",
            "|    policy_gradient_loss | -0.000756     |\n",
            "|    value_loss           | 15.3          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -50.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 159           |\n",
            "|    time_elapsed         | 2763          |\n",
            "|    total_timesteps      | 20352         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5171245e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.306        |\n",
            "|    explained_variance   | 0.00914       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 63.5          |\n",
            "|    n_updates            | 7820          |\n",
            "|    policy_gradient_loss | 0.000259      |\n",
            "|    value_loss           | 128           |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -51.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 160          |\n",
            "|    time_elapsed         | 2775         |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.076243e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.3         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.3         |\n",
            "|    n_updates            | 7830         |\n",
            "|    policy_gradient_loss | -0.000103    |\n",
            "|    value_loss           | 33.5         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -51.1         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 161           |\n",
            "|    time_elapsed         | 2796          |\n",
            "|    total_timesteps      | 20608         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7567072e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.291        |\n",
            "|    explained_variance   | 0.00487       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 17.1          |\n",
            "|    n_updates            | 7840          |\n",
            "|    policy_gradient_loss | -0.000199     |\n",
            "|    value_loss           | 34.8          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -50.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 162           |\n",
            "|    time_elapsed         | 2809          |\n",
            "|    total_timesteps      | 20736         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021150429 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.29         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.96          |\n",
            "|    n_updates            | 7850          |\n",
            "|    policy_gradient_loss | -0.000413     |\n",
            "|    value_loss           | 7.92          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -50.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 163           |\n",
            "|    time_elapsed         | 2829          |\n",
            "|    total_timesteps      | 20864         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015772786 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.304        |\n",
            "|    explained_variance   | -0.00746      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 12            |\n",
            "|    n_updates            | 7860          |\n",
            "|    policy_gradient_loss | -0.000893     |\n",
            "|    value_loss           | 24.7          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | -51.2        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 164          |\n",
            "|    time_elapsed         | 2842         |\n",
            "|    total_timesteps      | 20992        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.914511e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.32        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 40.2         |\n",
            "|    n_updates            | 7870         |\n",
            "|    policy_gradient_loss | -6.34e-05    |\n",
            "|    value_loss           | 80.6         |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -51.4         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 165           |\n",
            "|    time_elapsed         | 2865          |\n",
            "|    total_timesteps      | 21120         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.5976754e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.332        |\n",
            "|    explained_variance   | -0.0165       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.7           |\n",
            "|    n_updates            | 7880          |\n",
            "|    policy_gradient_loss | -0.000198     |\n",
            "|    value_loss           | 13.6          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | -51.4         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 166           |\n",
            "|    time_elapsed         | 2892          |\n",
            "|    total_timesteps      | 21248         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011364184 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.321        |\n",
            "|    explained_variance   | 0.00189       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 35.4          |\n",
            "|    n_updates            | 7890          |\n",
            "|    policy_gradient_loss | -0.00111      |\n",
            "|    value_loss           | 71            |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -51.8         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 167           |\n",
            "|    time_elapsed         | 2904          |\n",
            "|    total_timesteps      | 21376         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9631349e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.346        |\n",
            "|    explained_variance   | 0.000134      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 25.4          |\n",
            "|    n_updates            | 7900          |\n",
            "|    policy_gradient_loss | -0.000255     |\n",
            "|    value_loss           | 52            |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| rollout/                |                |\n",
            "|    ep_len_mean          | 216            |\n",
            "|    ep_rew_mean          | -51.8          |\n",
            "| time/                   |                |\n",
            "|    fps                  | 7              |\n",
            "|    iterations           | 168            |\n",
            "|    time_elapsed         | 2928           |\n",
            "|    total_timesteps      | 21504          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000118988566 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -0.349         |\n",
            "|    explained_variance   | 0.00452        |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 14.6           |\n",
            "|    n_updates            | 7910           |\n",
            "|    policy_gradient_loss | -0.000457      |\n",
            "|    value_loss           | 29.6           |\n",
            "--------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -52.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 169           |\n",
            "|    time_elapsed         | 2940          |\n",
            "|    total_timesteps      | 21632         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.2925513e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.363        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 22.4          |\n",
            "|    n_updates            | 7920          |\n",
            "|    policy_gradient_loss | 0.000855      |\n",
            "|    value_loss           | 46.6          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -52.5         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 170           |\n",
            "|    time_elapsed         | 2960          |\n",
            "|    total_timesteps      | 21760         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010252558 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.351        |\n",
            "|    explained_variance   | 0.0072        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 28.9          |\n",
            "|    n_updates            | 7930          |\n",
            "|    policy_gradient_loss | -0.000483     |\n",
            "|    value_loss           | 59.7          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -53           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 171           |\n",
            "|    time_elapsed         | 2971          |\n",
            "|    total_timesteps      | 21888         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6055612e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.339        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 30.5          |\n",
            "|    n_updates            | 7940          |\n",
            "|    policy_gradient_loss | -0.000232     |\n",
            "|    value_loss           | 63.1          |\n",
            "-------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -53.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 172           |\n",
            "|    time_elapsed         | 2996          |\n",
            "|    total_timesteps      | 22016         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.9444695e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.337        |\n",
            "|    explained_variance   | -0.0153       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 11.8          |\n",
            "|    n_updates            | 7950          |\n",
            "|    policy_gradient_loss | -0.00019      |\n",
            "|    value_loss           | 23.5          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -53.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 173          |\n",
            "|    time_elapsed         | 3021         |\n",
            "|    total_timesteps      | 22144        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.047008e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.344       |\n",
            "|    explained_variance   | 0.00851      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 65.1         |\n",
            "|    n_updates            | 7960         |\n",
            "|    policy_gradient_loss | 0.000165     |\n",
            "|    value_loss           | 131          |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | -53.3         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 7             |\n",
            "|    iterations           | 174           |\n",
            "|    time_elapsed         | 3034          |\n",
            "|    total_timesteps      | 22272         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2611039e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.345        |\n",
            "|    explained_variance   | 0.0016        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 19.1          |\n",
            "|    n_updates            | 7970          |\n",
            "|    policy_gradient_loss | -6.79e-05     |\n",
            "|    value_loss           | 39.1          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -53.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 175          |\n",
            "|    time_elapsed         | 3058         |\n",
            "|    total_timesteps      | 22400        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0121253785 |\n",
            "|    clip_fraction        | 0.0414       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.416       |\n",
            "|    explained_variance   | 0.00283      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 14           |\n",
            "|    n_updates            | 7980         |\n",
            "|    policy_gradient_loss | -0.0103      |\n",
            "|    value_loss           | 28           |\n",
            "------------------------------------------\n",
            "-------------end reward = -0.0004 / sum = -0.0004 dist = 0 movement_reward = 0 \n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | -52.7        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 7            |\n",
            "|    iterations           | 176          |\n",
            "|    time_elapsed         | 3070         |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017725923 |\n",
            "|    clip_fraction        | 0.00547      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.532       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.9          |\n",
            "|    n_updates            | 7990         |\n",
            "|    policy_gradient_loss | 0.00269      |\n",
            "|    value_loss           | 5.97         |\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################### TEST == SB3 ########################################\n",
        "model = RecurrentPPO.load(modelName, env=env)\n",
        "env = env = MyWayHomeGym(render=True, scenario=\"my_way_home\")\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "env.close()\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f}\")\n",
        "print(f\"std_reward:{std_reward:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SZuojiqP31f2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}